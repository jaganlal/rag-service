{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### https://medium.com/the-ai-forum/implementing-contextual-retrieval-in-rag-pipeline-8f1bc7cbd5e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import getpass\n",
    "from typing import List, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from langchain.retrievers import ContextualCompressionRetriever,BM25Retriever,EnsembleRetriever\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers.embeddings_redundant_filter import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "import time\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from whoosh.index import create_in, open_dir, exists_in\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.qparser import QueryParser\n",
    "from pydantic import Field\n",
    "from typing import List\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhooshIndexManager:\n",
    "    def __init__(self, index_dir: str = \"./storage/bm25_index\"):\n",
    "        \"\"\"\n",
    "        Initialize the Whoosh index manager.\n",
    "\n",
    "        Args:\n",
    "            index_dir (str): Directory to store the Whoosh index.\n",
    "        \"\"\"\n",
    "        self.index_dir = index_dir\n",
    "        self.schema = Schema(\n",
    "            doc_id=ID(stored=True, unique=True),\n",
    "            content=TEXT(stored=True)\n",
    "        )\n",
    "        self.index = self._initialize_index()\n",
    "\n",
    "    def _initialize_index(self):\n",
    "        \"\"\"Initialize or load the index.\"\"\"\n",
    "        if not os.path.exists(self.index_dir):\n",
    "            # Create the directory if it doesn't exist\n",
    "            os.makedirs(self.index_dir, exist_ok=True)\n",
    "            print(f\"Created directory: {self.index_dir}\")\n",
    "            # Create a new index\n",
    "            return create_in(self.index_dir, self.schema)\n",
    "        elif exists_in(self.index_dir):\n",
    "            # Open the existing index\n",
    "            print(f\"Loading existing index from: {self.index_dir}\")\n",
    "            return open_dir(self.index_dir)\n",
    "        else:\n",
    "            # Directory exists, but it's not a valid Whoosh index\n",
    "            raise ValueError(\n",
    "                f\"The directory '{self.index_dir}' exists but does not contain a valid Whoosh index. \"\n",
    "                \"Delete the directory or ensure it contains a valid index.\"\n",
    "            )\n",
    "\n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"Add documents to the index.\"\"\"\n",
    "        writer = self.index.writer()\n",
    "        for i, doc in enumerate(documents):\n",
    "            writer.add_document(doc_id=str(i), content=doc.page_content)\n",
    "        writer.commit()\n",
    "        print(f\"Added {len(documents)} documents to the index.\")\n",
    "\n",
    "    def search(self, query: str, top_n: int = 10) -> List[dict]:\n",
    "        \"\"\"Search the index.\"\"\"\n",
    "        with self.index.searcher() as searcher:\n",
    "            parser = QueryParser(\"content\", schema=self.schema)\n",
    "            parsed_query = parser.parse(query)\n",
    "            results = searcher.search(parsed_query, limit=top_n)\n",
    "            return [\n",
    "                {\"content\": result[\"content\"], \"doc_id\": result[\"doc_id\"]}\n",
    "                for result in results\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhooshRetriever(BaseRetriever):\n",
    "    index_manager: object = Field(...)\n",
    "    k: int = Field(default=10)\n",
    "\n",
    "    def __init__(self, index_manager, k: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize the Whoosh retriever.\n",
    "\n",
    "        Args:\n",
    "            index_manager (WhooshIndexManager): The Whoosh index manager.\n",
    "            k (int): Number of documents to retrieve.\n",
    "        \"\"\"\n",
    "        super().__init__(index_manager=index_manager,\n",
    "                         k=k)  # Initialize the BaseRetriever with Pydantic fields\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"Retrieve documents in LangChain-compatible format.\"\"\"\n",
    "        results = self.index_manager.search(query, top_n=self.k)\n",
    "        return [\n",
    "            Document(page_content=result[\"content\"], metadata={\n",
    "                     \"doc_id\": result[\"doc_id\"]})\n",
    "            for result in results\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage Class - FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFAISSStorage:\n",
    "    def __init__(self, storage_path: str, embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            storage_path (str): Path to the directory where the FAISS index will be saved/loaded.\n",
    "            embeddings: Embeddings model (e.g., OpenAIEmbeddings, SentenceTransformerEmbeddings).\n",
    "        \"\"\"\n",
    "        self.storage_path = storage_path\n",
    "        self.embeddings = embeddings\n",
    "        self.vectorstore = None\n",
    "\n",
    "    def _index_exists(self) -> bool:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            bool: True if the index exists, False otherwise.\n",
    "        \"\"\"\n",
    "        return os.path.exists(os.path.join(self.storage_path, \"index.faiss\"))\n",
    "\n",
    "    def _load_or_create_index(self):\n",
    "        if self._index_exists():\n",
    "            print(\"Loading existing FAISS index...\")\n",
    "            self.vectorstore = FAISS.load_local(self.storage_path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            print(\"Creating new FAISS index...\")\n",
    "            self.vectorstore = FAISS.from_texts([\"\"], self.embeddings, metadatas=[{}])\n",
    "            self.save_index()\n",
    "\n",
    "    def load_index(self):\n",
    "        \"\"\"\n",
    "        Raises:\n",
    "            ValueError: If the index does not exist.\n",
    "        \"\"\"\n",
    "        if not self._index_exists():\n",
    "            raise ValueError(\"FAISS index does not exist at the specified storage path.\")\n",
    "\n",
    "        print(\"Loading existing FAISS index...\")\n",
    "        self.vectorstore = FAISS.load_local(self.storage_path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    def get_index(self):\n",
    "      self.load_index()\n",
    "      return self.vectorstore\n",
    "\n",
    "    def add_documents(self, documents: list[Document]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            documents (list[Document]): List of Document objects to add to the index.\n",
    "        \"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            self._load_or_create_index()\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to the FAISS index...\")\n",
    "        self.vectorstore.add_documents(documents)\n",
    "        return self.vectorstore\n",
    "\n",
    "    def save_index(self):\n",
    "        if self.vectorstore is None:\n",
    "            raise ValueError(\n",
    "                \"FAISS index not initialized. Add documents first.\")\n",
    "\n",
    "        print(\"Saving FAISS index...\")\n",
    "        self.vectorstore.save_local(self.storage_path)\n",
    "\n",
    "    def query(self, query: str, k: int = 5) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query (str): The query string.\n",
    "            k (int): Number of documents to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            list[Document]: List of Document objects most similar to the query.\n",
    "        \"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            raise ValueError(\n",
    "                \"FAISS index not initialized. Add documents first.\")\n",
    "\n",
    "        print(f\"Querying FAISS index for: '{query}'\")\n",
    "        return self.vectorstore.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRAGPipeline:\n",
    "    def __init__(self, create_contextual_rag = False, vectorstore_path = \"./storage/faiss_local\"):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=800,\n",
    "            chunk_overlap=100,\n",
    "        )\n",
    "        #self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "        self.vectorstore_path = vectorstore_path\n",
    "        self.create_contextual_rag = create_contextual_rag\n",
    "        model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "        model_kwargs = {'device': 'cpu'}\n",
    "        encode_kwargs = {'normalize_embeddings': False}\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs\n",
    "        )\n",
    "        # self.llm = ChatOpenAI(\n",
    "        #     model=\"gpt-4o\",\n",
    "        #     temperature=0,\n",
    "        #     max_tokens=None,\n",
    "        #     timeout=None,\n",
    "        #     max_retries=2,\n",
    "        # )\n",
    "\n",
    "        # self.llm = ChatGroq(\n",
    "        #     model=\"llama-3.2-3b-preview\",\n",
    "        #     temperature=0,\n",
    "        #     max_tokens=None,\n",
    "        #     timeout=None,\n",
    "        #     max_retries=2,\n",
    "        # )\n",
    "\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"mixtral-8x7b-32768\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "        )\n",
    "        self.storage_class: MyFAISSStorage = MyFAISSStorage(self.vectorstore_path, self.embeddings)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "    def process_document(self, document: str) -> Tuple[List[Document], List[Document]]:\n",
    "        chunks = self.text_splitter.create_documents([document])\n",
    "        contextualized_chunks = self._generate_contextualized_chunks(document, chunks)\n",
    "        return chunks, contextualized_chunks\n",
    "\n",
    "    def split_document(self, document: str, max_retries: int = 1, delay: int = 60) -> Tuple[List[Document], List[Document]]:\n",
    "        chunks = self.text_splitter.split_documents(document)\n",
    "        print(f\"Total number of chunks in document: {len(chunks)}\")\n",
    "        contextualized_chunks = []\n",
    "        if self.create_contextual_rag:\n",
    "          contextualized_chunks = self._generate_contextualized_chunks(document, chunks, max_retries, delay)\n",
    "        return chunks, contextualized_chunks\n",
    "\n",
    "    def _generate_contextualized_chunks(self, document: str, chunks: List[Document], max_retries: int = 1, delay: int = 60) -> List[Document]:\n",
    "        contextualized_chunks = []\n",
    "        for chunk in chunks:\n",
    "            retries = 0\n",
    "            while retries <= max_retries:\n",
    "                try:\n",
    "                    context = self._generate_context(document, chunk.page_content)\n",
    "                    contextualized_content = f\"{context}\\n\\n{chunk.page_content}\"\n",
    "                    contextualized_chunks.append(Document(page_content=contextualized_content, metadata=chunk.metadata))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if \"rate limit\" in str(e).lower() or \"exceeded\" in str(e).lower() or \"quota\" in str(e).lower():\n",
    "                        retries += 1\n",
    "                        if retries > max_retries:\n",
    "                            print(f\"Max retries ({max_retries}) exceeded for chunk: {chunk.page_content[:50]}...\")\n",
    "                            raise e\n",
    "                        delay_with_randomness = delay + random.random()\n",
    "                        print(f\"Rate limit error: {e}. Retrying chunk in {delay_with_randomness:.2f} seconds...\")\n",
    "                        time.sleep(delay_with_randomness)\n",
    "                    else:\n",
    "                        print(f\"Error processing chunk: {chunk.page_content[:50]}... Error: {e}\")\n",
    "                        raise e\n",
    "        return contextualized_chunks\n",
    "\n",
    "    def _generate_context(self, document: str, chunk: str) -> str:\n",
    "        relevant_document = self._extract_relevant_part(document, chunk)\n",
    "\n",
    "        print(f\"Length of the relevant document: {len(relevant_document)}\")\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant specializing in document analysis. Your task is to provide brief, relevant context for a chunk of text from the whitepaper report.\n",
    "        Here is the whitepaper:\n",
    "        <document>\n",
    "        {document}\n",
    "        </document>\n",
    "\n",
    "        Here is the chunk we want to situate within the whole document::\n",
    "        <chunk>\n",
    "        {chunk}\n",
    "        </chunk>\n",
    "\n",
    "        Provide a concise context (2-3 sentences) for this chunk, considering the following guidelines:\n",
    "        Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\n",
    "\n",
    "        Context:\n",
    "        \"\"\")\n",
    "        messages = prompt.format_messages(document=relevant_document, chunk=chunk)\n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "    def _extract_relevant_part(self, document: List[Document], chunk: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract a relevant part of the document for context generation.\n",
    "        This reduces the number of tokens sent to the LLM.\n",
    "        \"\"\"\n",
    "        # Find the position of the chunk in the document\n",
    "        chunk_start = document[0].page_content.find(chunk)\n",
    "        chunk_end = chunk_start + len(chunk)\n",
    "        \n",
    "        # Extract a window of text around the chunk (e.g., 2048 characters before and after)\n",
    "        window_size = 2048\n",
    "        start = max(0, chunk_start - window_size)\n",
    "        end = min(len(document[0].page_content), chunk_end + window_size)\n",
    "        \n",
    "        return document[0].page_content[start:end]\n",
    "\n",
    "    def _extract_relevant_part_with_maxtokens(self, document: List[Document], chunk: str, max_tokens: int = 2048) -> str:\n",
    "        chunk_tokens = len(self.tokenizer.encode(chunk))\n",
    "        \n",
    "        # Calculate the maximum allowed tokens for context\n",
    "        max_context_tokens = max_tokens - chunk_tokens\n",
    "        \n",
    "        # Estimate the number of characters per token (average is ~4 characters per token)\n",
    "        chars_per_token = 4\n",
    "        max_context_chars = max_context_tokens * chars_per_token\n",
    "        \n",
    "        # Find the position of the chunk in the document\n",
    "        chunk_start = document[0].page_content.find(chunk)\n",
    "        chunk_end = chunk_start + len(chunk)\n",
    "        \n",
    "        # Extract a window of text around the chunk\n",
    "        window_size = min(max_context_chars, len(document[0].page_content))\n",
    "        start = max(0, chunk_start - window_size // 2)\n",
    "        end = min(len(document[0].page_content), chunk_end + window_size // 2)\n",
    "        \n",
    "        return document[0].page_content[start:end]\n",
    "\n",
    "    def create_inmemory_vectorstores(self, chunks: List[Document]) -> FAISS:\n",
    "        return FAISS.from_documents(chunks, self.embeddings)\n",
    "\n",
    "    def create_vectorstores(self, chunks: List[Document]) -> FAISS:\n",
    "        return self.storage_class.add_documents(chunks)\n",
    "\n",
    "    def get_vectorstores(self):\n",
    "        return self.storage_class.get_index()\n",
    "\n",
    "    def save_vectorstores(self):\n",
    "        self.storage_class.save_index()\n",
    "\n",
    "    def create_bm25_index(self, chunks: List[Document]) -> BM25Okapi:\n",
    "        tokenized_chunks = [chunk.page_content.split() for chunk in chunks]\n",
    "        return BM25Okapi(tokenized_chunks)\n",
    "    \n",
    "    def create_flashrank_index(self,vectorstore):\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\":20})\n",
    "        compression_retriever = ContextualCompressionRetriever(base_compressor=FlashrankRerank(), base_retriever=retriever)\n",
    "        return compression_retriever\n",
    "\n",
    "    def create_bm25_retriever(self, chunks: List[Document]) -> BM25Retriever:\n",
    "        bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "        return bm25_retriever\n",
    "    \n",
    "    def create_ensemble_retriever_reranker(self, vectorstore, bm25_retriever) -> EnsembleRetriever:\n",
    "        retriever_vs = vectorstore.as_retriever(search_kwargs={\"k\":20})\n",
    "        bm25_retriever.k =10\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[retriever_vs, bm25_retriever],\n",
    "            weights=[0.5, 0.5]\n",
    "        )\n",
    "        redundant_filter = EmbeddingsRedundantFilter(embeddings=self.embeddings)\n",
    "        reranker = FlashrankRerank()\n",
    "        pipeline_compressor = DocumentCompressorPipeline(transformers=[redundant_filter, reranker])\n",
    "        compression_pipeline = ContextualCompressionRetriever(base_compressor=pipeline_compressor,\n",
    "                                                      base_retriever=ensemble_retriever)\n",
    "        return compression_pipeline\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_cache_key(document: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a cache key for a document.\n",
    "        \"\"\"\n",
    "        return hashlib.md5(document.encode()).hexdigest()\n",
    "\n",
    "    def generate_answer(self, query: str, relevant_chunks: List[str]) -> str:\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Based on the following information, please provide a concise and accurate answer to the question.\n",
    "        If the information is not sufficient to answer the question, say so.\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Relevant information:\n",
    "        {chunks}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\")\n",
    "        messages = prompt.format_messages(query=query, chunks=\"\\n\\n\".join(relevant_chunks))\n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleRetrieverReranker:\n",
    "    def __init__(self, embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: Embeddings model (e.g., OpenAIEmbeddings, SentenceTransformerEmbeddings).\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def _convert_numpy_to_float(self, obj):\n",
    "        \"\"\"\n",
    "        Recursively converts numpy.float32 values in a dictionary, list, or other object to native Python float.\n",
    "\n",
    "        Args:\n",
    "            obj: A dictionary, list, or other object.\n",
    "\n",
    "        Returns:\n",
    "            The same object with numpy.float32 values converted to float.\n",
    "        \"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: self._convert_numpy_to_float(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_numpy_to_float(item) for item in obj]\n",
    "        elif isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            # Convert numpy arrays to lists of floats\n",
    "            return obj.astype(float).tolist()\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def _process_document(self, doc):\n",
    "        \"\"\"\n",
    "        Processes a Document object to ensure all numpy.float32 values are converted to float.\n",
    "\n",
    "        Args:\n",
    "            doc: A Document object.\n",
    "\n",
    "        Returns:\n",
    "            Document: A processed Document object.\n",
    "        \"\"\"\n",
    "        # Convert metadata\n",
    "        if hasattr(doc, \"metadata\") and doc.metadata:\n",
    "            doc.metadata = self._convert_numpy_to_float(doc.metadata)\n",
    "\n",
    "        # Convert embeddings (if present)\n",
    "        if hasattr(doc, \"embedding\") and isinstance(doc.embedding, (np.ndarray, np.float32)):\n",
    "            doc.embedding = self._convert_numpy_to_float(doc.embedding)\n",
    "\n",
    "        return doc\n",
    "\n",
    "    def create_ensemble_retriever_reranker(self, vectorstore, bm25_retriever):\n",
    "        \"\"\"\n",
    "        Creates an ensemble retriever with a reranker.\n",
    "\n",
    "        Args:\n",
    "            vectorstore: Initialized FAISS vectorstore.\n",
    "            bm25_retriever: BM25 retriever to combine with the FAISS retriever.\n",
    "\n",
    "        Returns:\n",
    "            ContextualCompressionRetriever: An ensemble retriever with a reranker.\n",
    "        \"\"\"\n",
    "        if vectorstore is None:\n",
    "            raise ValueError(\n",
    "                \"FAISS vectorstore is not initialized. Ensure the vectorstore is loaded or created.\")\n",
    "\n",
    "        # Ensure the vectorstore is initialized\n",
    "        retriever_vs = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "        bm25_retriever.k = 10\n",
    "\n",
    "        # Create ensemble retriever\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[retriever_vs, bm25_retriever],\n",
    "            weights=[0.5, 0.5]\n",
    "        )\n",
    "\n",
    "        # Add reranker and compression pipeline\n",
    "        redundant_filter = EmbeddingsRedundantFilter(\n",
    "            embeddings=self.embeddings)\n",
    "        reranker = FlashrankRerank()\n",
    "        pipeline_compressor = DocumentCompressorPipeline(\n",
    "            transformers=[redundant_filter, reranker])\n",
    "        compression_pipeline = ContextualCompressionRetriever(\n",
    "            base_compressor=pipeline_compressor,\n",
    "            base_retriever=ensemble_retriever\n",
    "        )\n",
    "\n",
    "        return compression_pipeline\n",
    "\n",
    "    def invoke(self, query, compression_pipeline):\n",
    "        \"\"\"\n",
    "        Invokes the ensemble retriever with reranker and preprocesses the results.\n",
    "\n",
    "        Args:\n",
    "            query: The query string.\n",
    "            compression_pipeline: The ensemble retriever with reranker.\n",
    "\n",
    "        Returns:\n",
    "            list[Document]: List of Document objects with all numpy.float32 values converted to float.\n",
    "        \"\"\"\n",
    "        # Use the `invoke` method to retrieve documents\n",
    "        docs = compression_pipeline.invoke(query)\n",
    "\n",
    "        # Process each document to ensure all numpy.float32 values are converted to float\n",
    "        processed_docs = [self._process_document(doc) for doc in docs]\n",
    "\n",
    "        return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "instruction = \"\"\"The provided document is a PDF file containing structured and unstructured content.\n",
    "It may include financial information, tables, management discussions, and analyses.\n",
    "Try to capture the essence of the document, including text, tables, and key highlights.\n",
    "Be precise and ensure data integrity while processing.\"\"\"\n",
    "\n",
    "\n",
    "async def parse_pdf(file_path: str):\n",
    "  parser = LlamaParse(\n",
    "      result_type=\"markdown\",\n",
    "      parsing_instruction=instruction,\n",
    "      max_timeout=5000,\n",
    "  )\n",
    "  return await parser.aload_data(file_path)\n",
    "\n",
    "\n",
    "async def load_and_combine_documents(folder_path: str, output_folder: str):\n",
    "  for filename in os.listdir(folder_path):\n",
    "    combined_content = \"\"\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if filename.endswith('.pdf'):\n",
    "        print(f\"Parsing {filename}...\")\n",
    "        parsed_data = await parse_pdf(file_path)\n",
    "        combined_content += f\"# Document: {filename}\\n\\n{parsed_data}\\n\\n\"\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {filename}\")\n",
    "    output_file = output_folder + \"/\" + os.path.splitext(filename)[0] + \".md\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as md_file:\n",
    "        md_file.write(combined_content)\n",
    "    print(f\"All documents combined into {output_file}\")\n",
    "\n",
    "\n",
    "def read_markdown_with_loader(folder_path: str):\n",
    "  documents = []\n",
    "  for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if filename.endswith('.md'):\n",
    "      loader = UnstructuredMarkdownLoader(file_path)\n",
    "      documents.append(loader.load())\n",
    "  return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Insatiate RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./dataset/source\"\n",
    "output_folder = \"./dataset/converted_md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await load_and_combine_documents(folder_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = read_markdown_with_loader(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_contextual_rag = False\n",
    "my_rag = MyRAGPipeline(create_contextual_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-3B-Instruct\")\n",
    "tpm_limit = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_chunks(document, max_retries=1, delay=60):\n",
    "    details = {}\n",
    "    source_document = document[0].metadata[\"source\"]\n",
    "    details[\"source\"] = source_document\n",
    "    details[\"original_chunks\"], details[\"contextualized_chunks\"] = my_rag.split_document(document, max_retries, delay)\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_manager = WhooshIndexManager(index_dir=\"./storage/bm25_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_token_limit(document, current_tokens, tpm_limit):\n",
    "    tokens = len(tokenizer.encode(str(document)))\n",
    "    current_tokens += tokens\n",
    "\n",
    "    if current_tokens >= tpm_limit:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document, retry_document_processing, current_tokens, delay=60):\n",
    "  try:\n",
    "    source_document = document[0].metadata[\"source\"]\n",
    "    print(f\"Processing document: {source_document} with currrent tokens: {current_tokens}\")\n",
    "    details = get_document_chunks(document, 1, delay)\n",
    "\n",
    "    if bool(details):\n",
    "        my_rag.create_vectorstores(details[\"original_chunks\"])\n",
    "        index_manager.add_documents(details[\"original_chunks\"])\n",
    "        if create_contextual_rag:\n",
    "            my_rag.create_vectorstores(details[\"contextualized_chunks\"])\n",
    "            index_manager.add_documents(details[\"contextualized_chunks\"])\n",
    "\n",
    "    if create_contextual_rag and not is_within_token_limit(details, current_tokens, tpm_limit):\n",
    "        print(f\"TPM limit reached, current tokens are: {current_tokens}. Waiting for {delay} seconds...\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return current_tokens\n",
    "\n",
    "  except Exception as e:\n",
    "      print(f\"Failed to process document: {source_document}. Error: {e}\")\n",
    "      if delay > 60: # Hack: it's retrying, don't append again\n",
    "        retry_document_processing.append(document)\n",
    "\n",
    "      return current_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_document_processing = []\n",
    "current_tokens = 0\n",
    "\n",
    "for document in documents:\n",
    "  current_tokens = process_document(document, retry_document_processing, current_tokens, 22)\n",
    "\n",
    "# Retry processing failed documents\n",
    "if retry_document_processing:\n",
    "    print(\"Retrying failed documents...\")\n",
    "    for document in retry_document_processing:\n",
    "        current_tokens = process_document(document, retry_document_processing, current_tokens, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rag.save_vectorstores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create retriver system with hybrid search with Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_manager = WhooshIndexManager(index_dir=\"./storage/bm25_index\")\n",
    "\n",
    "bm25_retriever = WhooshRetriever(index_manager=index_manager, k=10)\n",
    "vectorstore = my_rag.get_vectorstores()\n",
    "ensemble_retriever_reranker = my_rag.create_ensemble_retriever_reranker(vectorstore, bm25_retriever)\n",
    "\n",
    "# # Retrieve documents based on a query\n",
    "# query = \"second document\"\n",
    "# results = bm25_retriever._get_relevant_documents(query)\n",
    "# for result in results:\n",
    "#     print(result.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever_reranker.invoke(\"What is ViDoRe?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "ensemble_reranker = EnsembleRetrieverReranker(embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_pipeline = ensemble_reranker.create_ensemble_retriever_reranker(vectorstore, bm25_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is ViDoRe?\"\n",
    "results = ensemble_reranker.invoke(query, compression_pipeline)\n",
    "\n",
    "for doc in results:\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
