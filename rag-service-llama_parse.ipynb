{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvZGjmr9Rl6w",
        "outputId": "b4f1bae5-87e8-43b6-b648-0473d8738bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3.13\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ynuxRM7_-Nhu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkGT7MDh-mGk"
      },
      "source": [
        "###Call LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-G_ZBmiSJRP",
        "outputId": "d8563a20-61c0-46e4-a84f-43751ef9c115"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=128,\n",
        "    length_function=len\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_parse import LlamaParse\n",
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "instruction = \"\"\"The provided document is a PDF file containing structured and unstructured content.\n",
        "It may include financial information, tables, management discussions, and analyses.\n",
        "Try to capture the essence of the document, including text, tables, and key highlights.\n",
        "Be precise and ensure data integrity while processing.\"\"\"\n",
        "\n",
        "async def parse_pdf(file_path: str):\n",
        "  parser = LlamaParse(\n",
        "      result_type=\"markdown\",\n",
        "      parsing_instruction=instruction,\n",
        "      max_timeout=5000,\n",
        "  )\n",
        "  return await parser.aload_data(file_path)\n",
        "\n",
        "async def load_and_combine_documents(folder_path: str, output_file: str):\n",
        "  combined_content = \"\"\n",
        "  for filename in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    if filename.endswith('.pdf'):\n",
        "        print(f\"Parsing {filename}...\")\n",
        "        parsed_data = await parse_pdf(file_path)\n",
        "        combined_content += f\"# Document: {filename}\\n\\n{parsed_data}\\n\\n\"\n",
        "    else:\n",
        "        print(f\"Unsupported file type: {filename}\")\n",
        "  with open(output_file, \"w\", encoding=\"utf-8\") as md_file:\n",
        "      md_file.write(combined_content)\n",
        "  print(f\"All documents combined into {output_file}\")\n",
        "\n",
        "def read_markdown_with_loader(file_path: str):\n",
        "  loader = UnstructuredMarkdownLoader(file_path)\n",
        "  documents = loader.load()\n",
        "  return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever, BM25Retriever, EnsembleRetriever\n",
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "from langchain_community.document_transformers.embeddings_redundant_filter import EmbeddingsRedundantFilter\n",
        "from langchain.retrievers.document_compressors import FlashrankRerank\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "def create_bm25_index(chunks: List[Document]) -> BM25Okapi:\n",
        "  tokenized_chunks = [chunk.page_content.split() for chunk in chunks]\n",
        "  return BM25Okapi(tokenized_chunks)\n",
        "\n",
        "def create_bm25_retriever(chunks: List[Document]) -> BM25Retriever:\n",
        "  bm25_retriever = BM25Retriever.from_documents(chunks)\n",
        "  return bm25_retriever\n",
        "\n",
        "def create_flashrank_index(vectorstore):\n",
        "  retriever = vectorstore.as_retriever(search_kwargs={\"k\":20})\n",
        "  compression_retriever = ContextualCompressionRetriever(base_compressor=FlashrankRerank(), base_retriever=retriever)\n",
        "  return compression_retriever\n",
        "\n",
        "def create_ensemble_retriever_reranker(vectorstore, bm25_retriever, embeddings) -> EnsembleRetriever:\n",
        "  retriever_vs = vectorstore.as_retriever(search_kwargs={\"k\":20})\n",
        "  bm25_retriever.k =10\n",
        "  ensemble_retriever = EnsembleRetriever(\n",
        "      retrievers=[retriever_vs, bm25_retriever],\n",
        "      weights=[0.5, 0.5]\n",
        "  )\n",
        "  redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
        "\n",
        "  reranker = FlashrankRerank()\n",
        "  pipeline_compressor = DocumentCompressorPipeline(transformers=[redundant_filter, reranker])\n",
        "\n",
        "  compression_pipeline = ContextualCompressionRetriever(base_compressor=pipeline_compressor,\n",
        "                                                base_retriever=ensemble_retriever)\n",
        "  return compression_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsupported file type: combined_documents.md\n",
            "Parsing ColPali_2407.01449v3.pdf...\n",
            "Error while parsing the file './dataset/ColPali_2407.01449v3.pdf': Failed to parse the file: {\"detail\":\"Oops! Something went wrong on our end: Internal Server Error. Please try again in a few minutes. If the problem persists, please contact support by clicking the chat icon on cloud.llamaindex.ai providing this correlation ID: 1f123061-9c27-4526-b386-4ec866de72ea\"}\n",
            "All documents combined into ./dataset/combined_documents.md\n"
          ]
        }
      ],
      "source": [
        "folder_path = \"./dataset\"\n",
        "output_file = \"./dataset/combined_documents.md\"\n",
        "await load_and_combine_documents(folder_path, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents = read_markdown_with_loader(output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "splits = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgBuC-Xdu5gL"
      },
      "source": [
        "###Create and persist Chroma vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ov-ElhBUpJB",
        "outputId": "8fcd703f-120c-46db-ba55-2bc1423c4da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store created and persisted to './chroma_db'\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "embedding_function = OpenAIEmbeddings()\n",
        "collection_name = \"rag_service_collection_nb_llama_parse\"\n",
        "vectorstore = Chroma.from_documents(collection_name=collection_name, documents=splits, embedding=embedding_function, persist_directory=\"./chroma_db\")\n",
        "#db.persist()\n",
        "\n",
        "print(\"Vector store created and persisted to './chroma_db'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"How to understand documents visually?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaRqFWA8u3I8",
        "outputId": "31309e8a-a5b0-4d65-8b36-993e8fc440af"
      },
      "outputs": [],
      "source": [
        "# # 5. Perform similarity search\n",
        "# search_results = vectorstore.similarity_search(query, k=5)\n",
        "\n",
        "# print(f\"\\nTop 2 most relevant chunks for the query: '{query}'\\n\")\n",
        "# for i, result in enumerate(search_results, 1):\n",
        "#     print(f\"Result {i}:\")\n",
        "#     print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
        "#     print(f\"Content: {result.page_content}\")\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGkvzzIZvC2R",
        "outputId": "081f1019-f8a3-4dfa-9ed4-22f2d7198cfa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './dataset/combined_documents.md'}, page_content=\"for text understanding.\\\\n\\\\n2. Visually Rich Document Understanding:\\\\n- Models that encode text alongside visual features have been developed.\\\\n- Large Language Models (LLMs) are combined with Vision Transformers (ViTs) to enhance understanding.\\\\n\\\\n3. PaliGemma Model:\\\\n- A model that integrates visual and textual embeddings, fine-tuned for enhanced performance in tasks like Visual Question Answering and document understanding.\\\\n\\\\n### ViDoRe Benchmark\\\\n- Purpose: To evaluate retrieval systems' ability to match queries to relevant documents at the page level, addressing the gap in existing benchmarks that focus on either natural images or textual passages.\\\\n- Design: Includes various modalities (text, figures, tables) and thematic domains (e.g., medical).\\\\n\\\\nThis document emphasizes the importance of integrating visual features into retrieval systems to improve performance and better mimic human understanding of documents. The ViDoRe benchmark aims to provide a comprehensive evaluation framework for these\"),\n",
              " Document(metadata={'source': './dataset/combined_documents.md'}, page_content=\"Visually Rich Document Understanding:\\\\n- Models that encode text alongside visual features have been developed.\\\\n- Large Language Models (LLMs) are combined with Vision Transformers (ViTs) to enhance understanding.\\\\n\\\\n3. PaliGemma Model:\\\\n- A model that integrates visual and textual embeddings, fine-tuned for enhanced performance in tasks like Visual Question Answering and document understanding.\\\\n\\\\n### ViDoRe Benchmark\\\\n- Purpose: To evaluate retrieval systems' ability to match queries to relevant\"),\n",
              " Document(metadata={'source': './dataset/combined_documents.md'}, page_content=\"Visually Rich Document Understanding:\\\\n- Models that encode text alongside visual features have been developed.\\\\n- Large Language Models (LLMs) are combined with Vision Transformers (ViTs) to enhance understanding.\\\\n\\\\n3. PaliGemma Model:\\\\n- A model that integrates visual and textual embeddings, fine-tuned for enhanced performance in tasks like Visual Question Answering and document understanding.\\\\n\\\\n### ViDoRe Benchmark\\\\n- Purpose: To evaluate retrieval systems' ability to match queries to relevant\"),\n",
              " Document(metadata={'source': './dataset/combined_documents.md'}, page_content=\"Visually Rich Document Understanding:\\\\n- Models that encode text alongside visual features have been developed.\\\\n- Large Language Models (LLMs) are combined with Vision Transformers (ViTs) to enhance understanding.\\\\n\\\\n3. PaliGemma Model:\\\\n- A model that integrates visual and textual embeddings, fine-tuned for enhanced performance in tasks like Visual Question Answering and document understanding.\\\\n\\\\n### ViDoRe Benchmark\\\\n- Purpose: To evaluate retrieval systems' ability to match queries to relevant\"),\n",
              " Document(metadata={'source': './dataset/combined_documents.md'}, page_content='considers visual elements.\\\\n- ColPali Model: A novel architecture that leverages Vision Language Models for better document understanding and retrieval efficiency.\\\\n\\\\n----\\\\n\\\\nFigure 1 Description:\\\\nThe figure illustrates how ColPali identifies relevant document image patches in response to user queries, highlighting the areas of interest and computing matching scores for efficient retrieval from a pre-indexed corpus.\\\\n\\\\n----\\\\n\\\\n2019 Average Hourly Generation by Fuel Type:\\\\n- A table or graph (not fully'),\n",
              " Document(metadata={'source': './dataset/combined_documents.md'}, page_content='considers visual elements.\\\\n- ColPali Model: A novel architecture that leverages Vision Language Models for better document understanding and retrieval efficiency.\\\\n\\\\n----\\\\n\\\\nFigure 1 Description:\\\\nThe figure illustrates how ColPali identifies relevant document image patches in response to user queries, highlighting the areas of interest and computing matching scores for efficient retrieval from a pre-indexed corpus.\\\\n\\\\n----\\\\n\\\\n2019 Average Hourly Generation by Fuel Type:\\\\n- A table or graph (not fully'),\n",
              " Document(metadata={'source': './dataset/combined_documents.md'}, page_content='considers visual elements.\\\\n- ColPali Model: A novel architecture that leverages Vision Language Models for better document understanding and retrieval efficiency.\\\\n\\\\n----\\\\n\\\\nFigure 1 Description:\\\\nThe figure illustrates how ColPali identifies relevant document image patches in response to user queries, highlighting the areas of interest and computing matching scores for efficient retrieval from a pre-indexed corpus.\\\\n\\\\n----\\\\n\\\\n2019 Average Hourly Generation by Fuel Type:\\\\n- A table or graph (not fully'),\n",
              " Document(metadata={'source': './dataset/combined_documents.md'}, page_content='to match queries to relevant documents at the page level, addressing the gap in existing benchmarks that focus on either natural images or textual passages.\\\\n- Design: Includes various modalities (text, figures, tables) and thematic domains (e.g., medical).\\\\n\\\\nThis document emphasizes the importance of integrating visual features into retrieval systems to improve performance and better mimic human understanding of documents. The ViDoRe benchmark aims to provide a comprehensive evaluation framework for'),\n",
              " Document(metadata={'source': './dataset/combined_documents.md'}, page_content='to match queries to relevant documents at the page level, addressing the gap in existing benchmarks that focus on either natural images or textual passages.\\\\n- Design: Includes various modalities (text, figures, tables) and thematic domains (e.g., medical).\\\\n\\\\nThis document emphasizes the importance of integrating visual features into retrieval systems to improve performance and better mimic human understanding of documents. The ViDoRe benchmark aims to provide a comprehensive evaluation framework for'),\n",
              " Document(metadata={'source': './dataset/combined_documents.md'}, page_content='to match queries to relevant documents at the page level, addressing the gap in existing benchmarks that focus on either natural images or textual passages.\\\\n- Design: Includes various modalities (text, figures, tables) and thematic domains (e.g., medical).\\\\n\\\\nThis document emphasizes the importance of integrating visual features into retrieval systems to improve performance and better mimic human understanding of documents. The ViDoRe benchmark aims to provide a comprehensive evaluation framework for')]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain.retrievers import ContextualCompressionRetriever\n",
        "# from langchain.retrievers.document_compressors import FlashrankRerank\n",
        "\n",
        "# compressor = FlashrankRerank(model=\"ms-marco-MiniLM-L-12-v2\")\n",
        "# compression_retriever = ContextualCompressionRetriever(\n",
        "#     base_compressor=compressor, base_retriever=retriever\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_bm25_index = create_bm25_index(splits)\n",
        "original_reranker = create_flashrank_index(vectorstore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25_retriever_original = create_bm25_retriever(splits)\n",
        "original_ensemble_retriever_reranker = create_ensemble_retriever_reranker(vectorstore, bm25_retriever_original, embedding_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template = \"\"\"You are a highly capable assistant specializing in answering questions from visually rich documents. Consider both textual and visual elements as context.\n",
        "\n",
        "Given the context below:\n",
        "{context}\n",
        "\n",
        "And the question:\n",
        "{question}\n",
        "\n",
        "Provide a precise and concise answer based solely on the provided context. Do not include any information that is not explicitly present in the context.\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYCoxqbcvuht",
        "outputId": "aed01866-ba79-4685-cc45-9a060686983d"
      },
      "outputs": [],
      "source": [
        "# from langchain.schema.runnable import RunnablePassthrough\n",
        "# rag_chain = (\n",
        "#     {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt\n",
        "# )\n",
        "# rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uztRXXwDvud9"
      },
      "outputs": [],
      "source": [
        "def docs2str(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk-VtijcwZw9",
        "outputId": "b976c90d-5d8e-427b-cee7-2281167b746c"
      },
      "outputs": [],
      "source": [
        "# rag_chain = (\n",
        "#   {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()} | prompt\n",
        "# )\n",
        "# rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gjDAr3lwbXA",
        "outputId": "7a9df563-e772-401a-be4f-7abecb58ce40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To understand documents visually, it is important to integrate visual features alongside text. This can be achieved by using models that encode both text and visual elements, such as combining Large Language Models (LLMs) with Vision Transformers (ViTs). The PaliGemma model, for instance, integrates visual and textual embeddings to enhance tasks like Visual Question Answering and document understanding. Additionally, benchmarks like ViDoRe are designed to evaluate retrieval systems' ability to match queries to relevant documents at the page level, emphasizing the importance of processing visual components like figures and tables.\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "rag_chain = (\n",
        "    {\"context\": original_ensemble_retriever_reranker | docs2str, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "question = query\n",
        "response = rag_chain.invoke(question)\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
