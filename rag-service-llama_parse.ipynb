{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvZGjmr9Rl6w",
        "outputId": "b4f1bae5-87e8-43b6-b648-0473d8738bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3.13\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ynuxRM7_-Nhu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkGT7MDh-mGk"
      },
      "source": [
        "###Call LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-G_ZBmiSJRP",
        "outputId": "d8563a20-61c0-46e4-a84f-43751ef9c115"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=32,\n",
        "    length_function=len\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_parse import LlamaParse\n",
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "instruction = \"\"\"The provided document is a PDF file containing structured and unstructured content.\n",
        "It may include financial information, tables, management discussions, and analyses.\n",
        "Try to capture the essence of the document, including text, tables, and key highlights.\n",
        "Be precise and ensure data integrity while processing.\"\"\"\n",
        "\n",
        "async def parse_pdf(file_path: str):\n",
        "  parser = LlamaParse(\n",
        "      result_type=\"markdown\",\n",
        "      parsing_instruction=instruction,\n",
        "      max_timeout=5000,\n",
        "  )\n",
        "  return await parser.aload_data(file_path)\n",
        "\n",
        "async def load_and_combine_documents(folder_path: str, output_file: str):\n",
        "  combined_content = \"\"\n",
        "  for filename in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    if filename.endswith('.pdf'):\n",
        "        print(f\"Parsing {filename}...\")\n",
        "        parsed_data = await parse_pdf(file_path)\n",
        "        combined_content += f\"# Document: {filename}\\n\\n{parsed_data}\\n\\n\"\n",
        "    else:\n",
        "        print(f\"Unsupported file type: {filename}\")\n",
        "  with open(output_file, \"w\", encoding=\"utf-8\") as md_file:\n",
        "      md_file.write(combined_content)\n",
        "  print(f\"All documents combined into {output_file}\")\n",
        "\n",
        "def read_markdown_with_loader(file_path: str):\n",
        "  loader = UnstructuredMarkdownLoader(file_path)\n",
        "  documents = loader.load()\n",
        "  return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing ColPali_2407.01449v3.pdf...\n",
            "Started parsing the file under job_id efadb68a-cc72-4f97-8c49-a9cf299b9954\n",
            "All documents combined into ./dataset/combined_documents.md\n"
          ]
        }
      ],
      "source": [
        "folder_path = \"./dataset\"\n",
        "output_file = \"./dataset/combined_documents.md\"\n",
        "await load_and_combine_documents(folder_path, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents = read_markdown_with_loader(output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "splits = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgBuC-Xdu5gL"
      },
      "source": [
        "###Create and persist Chroma vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ov-ElhBUpJB",
        "outputId": "8fcd703f-120c-46db-ba55-2bc1423c4da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store created and persisted to './chroma_db'\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "embedding_function = OpenAIEmbeddings()\n",
        "collection_name = \"rag_service_collection_nb_llama_parse\"\n",
        "vectorstore = Chroma.from_documents(collection_name=collection_name, documents=splits, embedding=embedding_function, persist_directory=\"./chroma_db\")\n",
        "#db.persist()\n",
        "\n",
        "print(\"Vector store created and persisted to './chroma_db'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"How to understand documents visually?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaRqFWA8u3I8",
        "outputId": "31309e8a-a5b0-4d65-8b36-993e8fc440af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 2 most relevant chunks for the query: 'How to understand documents visually?'\n",
            "\n",
            "Result 1:\n",
            "Source: ./dataset/combined_documents.md\n",
            "Content: Visually Rich Document Understanding:\\n- Models that encode text alongside visual features have been developed.\\n- Large Language Models (LLMs) are combined with Vision Transformers (ViTs) to enhance understanding.\\n\\n3. PaliGemma Model:\\n- A model that integrates visual and textual embeddings, fine-tuned for enhanced performance in tasks like Visual Question Answering and document understanding.\\n\\n### ViDoRe Benchmark\\n- Purpose: To evaluate retrieval systems' ability to match queries to relevant\n",
            "\n",
            "Result 2:\n",
            "Source: ./dataset/combined_documents.md\n",
            "Content: considers visual elements.\\n- ColPali Model: A novel architecture that leverages Vision Language Models for better document understanding and retrieval efficiency.\\n\\n----\\n\\nFigure 1 Description:\\nThe figure illustrates how ColPali identifies relevant document image patches in response to user queries, highlighting the areas of interest and computing matching scores for efficient retrieval from a pre-indexed corpus.\\n\\n----\\n\\n2019 Average Hourly Generation by Fuel Type:\\n- A table or graph (not fully\n",
            "\n",
            "Result 3:\n",
            "Source: ./dataset/combined_documents.md\n",
            "Content: to match queries to relevant documents at the page level, addressing the gap in existing benchmarks that focus on either natural images or textual passages.\\n- Design: Includes various modalities (text, figures, tables) and thematic domains (e.g., medical).\\n\\nThis document emphasizes the importance of integrating visual features into retrieval systems to improve performance and better mimic human understanding of documents. The ViDoRe benchmark aims to provide a comprehensive evaluation framework for\n",
            "\n",
            "Result 4:\n",
            "Source: ./dataset/combined_documents.md\n",
            "Content: output by integrating visual elements through OCR and captioning strategies, although these methods may increase latency and resource costs.\\n- Embedding Models: It evaluates different embedding methods, including Okapi BM25 and BGE-M3, to score and embed textual chunks.\\n\\n### Conclusion\\nThe document emphasizes the importance of integrating visual elements into retrieval systems and the need for high-quality datasets to improve performance in multimodal retrieval tasks. It also highlights the challenges\n",
            "\n",
            "Result 5:\n",
            "Source: ./dataset/combined_documents.md\n",
            "Content: Bilel Omrani (Illuin Technology)\\n- Gautier Viaud (Illuin Technology)\\n- Céline Hudelot (CentraleSupélec, Paris-Saclay)\\n- Pierre Colombo (Equall.ai, CentraleSupélec, Paris-Saclay)\\n\\nContact: manuel.faysse@centralesupelec.fr\\n\\n----\\n\\nAbstract:\\nThe paper discusses the challenges faced by modern document retrieval systems, particularly their inability to effectively utilize visual cues from documents. To address this, the authors introduce the Visual Document Retrieval Benchmark (ViDoRe), which includes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5. Perform similarity search\n",
        "search_results = vectorstore.similarity_search(query, k=5)\n",
        "\n",
        "print(f\"\\nTop 2 most relevant chunks for the query: '{query}'\\n\")\n",
        "for i, result in enumerate(search_results, 1):\n",
        "    print(f\"Result {i}:\")\n",
        "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
        "    print(f\"Content: {result.page_content}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGkvzzIZvC2R",
        "outputId": "081f1019-f8a3-4dfa-9ed4-22f2d7198cfa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './dataset/combined_documents.md'}, page_content=\"Visually Rich Document Understanding:\\\\n- Models that encode text alongside visual features have been developed.\\\\n- Large Language Models (LLMs) are combined with Vision Transformers (ViTs) to enhance understanding.\\\\n\\\\n3. PaliGemma Model:\\\\n- A model that integrates visual and textual embeddings, fine-tuned for enhanced performance in tasks like Visual Question Answering and document understanding.\\\\n\\\\n### ViDoRe Benchmark\\\\n- Purpose: To evaluate retrieval systems' ability to match queries to relevant\"),\n",
              " Document(metadata={'source': './dataset/combined_documents.md'}, page_content='considers visual elements.\\\\n- ColPali Model: A novel architecture that leverages Vision Language Models for better document understanding and retrieval efficiency.\\\\n\\\\n----\\\\n\\\\nFigure 1 Description:\\\\nThe figure illustrates how ColPali identifies relevant document image patches in response to user queries, highlighting the areas of interest and computing matching scores for efficient retrieval from a pre-indexed corpus.\\\\n\\\\n----\\\\n\\\\n2019 Average Hourly Generation by Fuel Type:\\\\n- A table or graph (not fully'),\n",
              " Document(metadata={'source': './dataset/combined_documents.md'}, page_content='to match queries to relevant documents at the page level, addressing the gap in existing benchmarks that focus on either natural images or textual passages.\\\\n- Design: Includes various modalities (text, figures, tables) and thematic domains (e.g., medical).\\\\n\\\\nThis document emphasizes the importance of integrating visual features into retrieval systems to improve performance and better mimic human understanding of documents. The ViDoRe benchmark aims to provide a comprehensive evaluation framework for'),\n",
              " Document(metadata={'source': './dataset/combined_documents.md'}, page_content='output by integrating visual elements through OCR and captioning strategies, although these methods may increase latency and resource costs.\\\\n- Embedding Models: It evaluates different embedding methods, including Okapi BM25 and BGE-M3, to score and embed textual chunks.\\\\n\\\\n### Conclusion\\\\nThe document emphasizes the importance of integrating visual elements into retrieval systems and the need for high-quality datasets to improve performance in multimodal retrieval tasks. It also highlights the challenges'),\n",
              " Document(metadata={'source': './dataset/combined_documents.md'}, page_content='Bilel Omrani (Illuin Technology)\\\\n- Gautier Viaud (Illuin Technology)\\\\n- Céline Hudelot (CentraleSupélec, Paris-Saclay)\\\\n- Pierre Colombo (Equall.ai, CentraleSupélec, Paris-Saclay)\\\\n\\\\nContact: manuel.faysse@centralesupelec.fr\\\\n\\\\n----\\\\n\\\\nAbstract:\\\\nThe paper discusses the challenges faced by modern document retrieval systems, particularly their inability to effectively utilize visual cues from documents. To address this, the authors introduce the Visual Document Retrieval Benchmark (ViDoRe), which includes')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template = \"\"\"You are a highly capable assistant specializing in answering questions from visually rich documents. Consider both textual and visual elements as context.\n",
        "\n",
        "Given the context below:\n",
        "{context}\n",
        "\n",
        "And the question:\n",
        "{question}\n",
        "\n",
        "Provide a precise and concise answer based solely on the provided context. Do not include any information that is not explicitly present in the context.\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYCoxqbcvuht",
        "outputId": "aed01866-ba79-4685-cc45-9a060686983d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='You are a highly capable assistant specializing in answering questions from visually rich documents. Consider both textual and visual elements as context.\\n\\nGiven the context below:\\n[Document(metadata={\\'source\\': \\'./dataset/combined_documents.md\\'}, page_content=\"Visually Rich Document Understanding:\\\\\\\\n- Models that encode text alongside visual features have been developed.\\\\\\\\n- Large Language Models (LLMs) are combined with Vision Transformers (ViTs) to enhance understanding.\\\\\\\\n\\\\\\\\n3. PaliGemma Model:\\\\\\\\n- A model that integrates visual and textual embeddings, fine-tuned for enhanced performance in tasks like Visual Question Answering and document understanding.\\\\\\\\n\\\\\\\\n### ViDoRe Benchmark\\\\\\\\n- Purpose: To evaluate retrieval systems\\' ability to match queries to relevant\"), Document(metadata={\\'source\\': \\'./dataset/combined_documents.md\\'}, page_content=\\'considers visual elements.\\\\\\\\n- ColPali Model: A novel architecture that leverages Vision Language Models for better document understanding and retrieval efficiency.\\\\\\\\n\\\\\\\\n----\\\\\\\\n\\\\\\\\nFigure 1 Description:\\\\\\\\nThe figure illustrates how ColPali identifies relevant document image patches in response to user queries, highlighting the areas of interest and computing matching scores for efficient retrieval from a pre-indexed corpus.\\\\\\\\n\\\\\\\\n----\\\\\\\\n\\\\\\\\n2019 Average Hourly Generation by Fuel Type:\\\\\\\\n- A table or graph (not fully\\'), Document(metadata={\\'source\\': \\'./dataset/combined_documents.md\\'}, page_content=\\'to match queries to relevant documents at the page level, addressing the gap in existing benchmarks that focus on either natural images or textual passages.\\\\\\\\n- Design: Includes various modalities (text, figures, tables) and thematic domains (e.g., medical).\\\\\\\\n\\\\\\\\nThis document emphasizes the importance of integrating visual features into retrieval systems to improve performance and better mimic human understanding of documents. The ViDoRe benchmark aims to provide a comprehensive evaluation framework for\\'), Document(metadata={\\'source\\': \\'./dataset/combined_documents.md\\'}, page_content=\\'output by integrating visual elements through OCR and captioning strategies, although these methods may increase latency and resource costs.\\\\\\\\n- Embedding Models: It evaluates different embedding methods, including Okapi BM25 and BGE-M3, to score and embed textual chunks.\\\\\\\\n\\\\\\\\n### Conclusion\\\\\\\\nThe document emphasizes the importance of integrating visual elements into retrieval systems and the need for high-quality datasets to improve performance in multimodal retrieval tasks. It also highlights the challenges\\'), Document(metadata={\\'source\\': \\'./dataset/combined_documents.md\\'}, page_content=\\'Bilel Omrani (Illuin Technology)\\\\\\\\n- Gautier Viaud (Illuin Technology)\\\\\\\\n- Céline Hudelot (CentraleSupélec, Paris-Saclay)\\\\\\\\n- Pierre Colombo (Equall.ai, CentraleSupélec, Paris-Saclay)\\\\\\\\n\\\\\\\\nContact: manuel.faysse@centralesupelec.fr\\\\\\\\n\\\\\\\\n----\\\\\\\\n\\\\\\\\nAbstract:\\\\\\\\nThe paper discusses the challenges faced by modern document retrieval systems, particularly their inability to effectively utilize visual cues from documents. To address this, the authors introduce the Visual Document Retrieval Benchmark (ViDoRe), which includes\\')]\\n\\nAnd the question:\\nHow to understand documents visually?\\n\\nProvide a precise and concise answer based solely on the provided context. Do not include any information that is not explicitly present in the context.\\n\\nAnswer:', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt\n",
        ")\n",
        "rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uztRXXwDvud9"
      },
      "outputs": [],
      "source": [
        "def docs2str(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk-VtijcwZw9",
        "outputId": "b976c90d-5d8e-427b-cee7-2281167b746c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content=\"You are a highly capable assistant specializing in answering questions from visually rich documents. Consider both textual and visual elements as context.\\n\\nGiven the context below:\\nVisually Rich Document Understanding:\\\\n- Models that encode text alongside visual features have been developed.\\\\n- Large Language Models (LLMs) are combined with Vision Transformers (ViTs) to enhance understanding.\\\\n\\\\n3. PaliGemma Model:\\\\n- A model that integrates visual and textual embeddings, fine-tuned for enhanced performance in tasks like Visual Question Answering and document understanding.\\\\n\\\\n### ViDoRe Benchmark\\\\n- Purpose: To evaluate retrieval systems' ability to match queries to relevant\\n\\nconsiders visual elements.\\\\n- ColPali Model: A novel architecture that leverages Vision Language Models for better document understanding and retrieval efficiency.\\\\n\\\\n----\\\\n\\\\nFigure 1 Description:\\\\nThe figure illustrates how ColPali identifies relevant document image patches in response to user queries, highlighting the areas of interest and computing matching scores for efficient retrieval from a pre-indexed corpus.\\\\n\\\\n----\\\\n\\\\n2019 Average Hourly Generation by Fuel Type:\\\\n- A table or graph (not fully\\n\\nto match queries to relevant documents at the page level, addressing the gap in existing benchmarks that focus on either natural images or textual passages.\\\\n- Design: Includes various modalities (text, figures, tables) and thematic domains (e.g., medical).\\\\n\\\\nThis document emphasizes the importance of integrating visual features into retrieval systems to improve performance and better mimic human understanding of documents. The ViDoRe benchmark aims to provide a comprehensive evaluation framework for\\n\\noutput by integrating visual elements through OCR and captioning strategies, although these methods may increase latency and resource costs.\\\\n- Embedding Models: It evaluates different embedding methods, including Okapi BM25 and BGE-M3, to score and embed textual chunks.\\\\n\\\\n### Conclusion\\\\nThe document emphasizes the importance of integrating visual elements into retrieval systems and the need for high-quality datasets to improve performance in multimodal retrieval tasks. It also highlights the challenges\\n\\nBilel Omrani (Illuin Technology)\\\\n- Gautier Viaud (Illuin Technology)\\\\n- Céline Hudelot (CentraleSupélec, Paris-Saclay)\\\\n- Pierre Colombo (Equall.ai, CentraleSupélec, Paris-Saclay)\\\\n\\\\nContact: manuel.faysse@centralesupelec.fr\\\\n\\\\n----\\\\n\\\\nAbstract:\\\\nThe paper discusses the challenges faced by modern document retrieval systems, particularly their inability to effectively utilize visual cues from documents. To address this, the authors introduce the Visual Document Retrieval Benchmark (ViDoRe), which includes\\n\\nAnd the question:\\nHow to understand documents visually?\\n\\nProvide a precise and concise answer based solely on the provided context. Do not include any information that is not explicitly present in the context.\\n\\nAnswer:\", additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain = (\n",
        "  {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()} | prompt\n",
        ")\n",
        "rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gjDAr3lwbXA",
        "outputId": "7a9df563-e772-401a-be4f-7abecb58ce40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Understanding documents visually involves integrating visual features with textual information to enhance retrieval systems. This is achieved through models like the PaliGemma and ColPali, which utilize visual and textual embeddings to improve performance in tasks such as Visual Question Answering and document understanding. Techniques such as Optical Character Recognition (OCR) and captioning are employed to extract and process visual elements, enabling systems to identify relevant document image patches in response to queries. The ViDoRe benchmark serves as a framework for evaluating the effectiveness of these multimodal retrieval systems.\n"
          ]
        }
      ],
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "question = query\n",
        "response = rag_chain.invoke(question)\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
