{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import IFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvZGjmr9Rl6w",
        "outputId": "b4f1bae5-87e8-43b6-b648-0473d8738bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3.13\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ynuxRM7_-Nhu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "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\n"
          ]
        }
      ],
      "source": [
        "print(os.environ.get('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkGT7MDh-mGk"
      },
      "source": [
        "###Call LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFPaXjak-Dx6",
        "outputId": "f7fadda0-e604-462d-e61b-42e2d40ead86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Why don't skeletons fight each other?\\n\\nBecause they don't have the guts!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 11, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-3fb3f09f-1718-4a6c-9dae-8bdeeac25dc7-0', usage_metadata={'input_tokens': 11, 'output_tokens': 16, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "llm_response = llm.invoke(\"Tell me a joke\")\n",
        "\n",
        "llm_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv-79m-P_kVF"
      },
      "source": [
        "###Parsing Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g1e1HTs1-vmG",
        "outputId": "b910f96b-8542-4a9f-9274-1a4f31ea1dfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Why don't skeletons fight each other?\\n\\nBecause they don't have the guts!\""
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "output_parser.invoke(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIWdUqOe_nIc"
      },
      "source": [
        "###Simple Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dZAu9MKc_S5z",
        "outputId": "990ebf8f-c4b1-4d98-a542-c06e5f509c80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Why don't skeletons fight each other?\\n\\nThey don't have the guts!\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = llm | output_parser\n",
        "chain.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIRFAy-R57iq"
      },
      "source": [
        "###Structured Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G41Ztxd541C",
        "outputId": "7fec3837-7f36-446e-bc50-f4f7537c5414"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MobileReview(phone_model='Samsung Galaxy S21', rating=4.0, pros=['Gorgeous screen', 'Vibrant colors', 'Insane camera quality, especially at night', 'Solid battery life'], cons=['Pretty pricey', 'No charger included', 'New button layout is confusing, often hitting Bixby accidentally'], summary=\"Great phone overall with a few annoying quirks, definitely worth checking out if you're due for an upgrade!\")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class MobileReview(BaseModel):\n",
        "    phone_model: str = Field(description=\"Name and model of the phone\")\n",
        "    rating: float = Field(description=\"Overall rating out of 5\")\n",
        "    pros: List[str] = Field(description=\"List of positive aspects\")\n",
        "    cons: List[str] = Field(description=\"List of negative aspects\")\n",
        "    summary: str = Field(description=\"Brief summary of the review\")\n",
        "\n",
        "review_text = \"\"\"\n",
        "Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous,\n",
        "colors pop like crazy. Camera's insane too, especially at night - my Insta game's never been\n",
        "stronger. Battery life's solid, lasts me all day no problem.\n",
        "\n",
        "Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.\n",
        "Also, still getting used to the new button layout, keep hitting Bixby by mistake.\n",
        "\n",
        "Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from\n",
        "being perfect. If you're due for an upgrade, definitely worth checking out!\n",
        "\"\"\"\n",
        "\n",
        "structured_llm = llm.with_structured_output(MobileReview)\n",
        "output = structured_llm.invoke(review_text)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB-1N7n7n9cz",
        "outputId": "ed375a70-2d1f-4ac5-8d07-ba44561c61be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Gorgeous screen',\n",
              " 'Vibrant colors',\n",
              " 'Insane camera quality, especially at night',\n",
              " 'Solid battery life']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.pros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpRopsnw_3yo"
      },
      "source": [
        "###Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH9TLudu_-Dw",
        "outputId": "0bd110f8-6b45-43ef-b5bb-2f7baa12c396"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='Tell me a short joke about programming', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
        "prompt.invoke({\"topic\": \"programming\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RakPrAV0BqlL",
        "outputId": "9e8c870a-0b12-4532-cd7e-52bd8503d3f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Why do programmers prefer dark mode? \\n\\nBecause light attracts bugs!'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = prompt | llm | output_parser\n",
        "chain.invoke({\"topic\": \"programer\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pizjNr0h8f_E",
        "outputId": "2ab1230f-7d5c-4ffb-c9d7-0c0f79bffe8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why do programmers prefer dark mode?\n",
            "\n",
            "Because light attracts bugs!\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define the prompt\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Define the output parser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Compose the chain\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "# Use the chain\n",
        "result = chain.invoke({\"topic\": \"programming\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59Hl3OjvsA02"
      },
      "source": [
        "###LLM Messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37riQcz1FoB_",
        "outputId": "3227525d-8f0b-469d-ac4b-3760895ce870"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure! Programming is the process of creating a set of instructions that a computer can follow to perform specific tasks. It involves writing code in various programming languages like Python, Java, C++, and many others. \\n\\nAnd speaking of programming, here's a joke for you:\\n\\nWhy do programmers prefer dark mode?\\n\\nBecause light attracts bugs!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 24, 'total_tokens': 89, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-e8b9a539-9bdb-47d2-8120-4684bbdf258e-0', usage_metadata={'input_tokens': 24, 'output_tokens': 65, 'total_tokens': 89, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "system_message = SystemMessage(content=\"You are a helpful assistant that tells jokes.\")\n",
        "human_message = HumanMessage(content=\"Tell me about programming\")\n",
        "llm.invoke([system_message, human_message])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEufVkCLMm68",
        "outputId": "f8f21040-feae-4a1c-a4c3-297c87d2c9ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that tells jokes.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about programming', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
        "    (\"human\", \"Tell me about {topic}\")\n",
        "])\n",
        "\n",
        "prompt_value = template.invoke(\n",
        "    {\n",
        "        \"topic\": \"programming\"\n",
        "    }\n",
        ")\n",
        "prompt_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY86l3k_HwTb",
        "outputId": "995734c5-18f8-4beb-872b-ae9702bd6044"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Sure! Here’s a light-hearted joke about programming:\\n\\nWhy do programmers prefer dark mode?\\n\\nBecause light attracts bugs! \\n\\nIf you want to know more about programming concepts or languages, feel free to ask!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 24, 'total_tokens': 65, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-808f0c38-920d-4d5a-99fb-de6f0765b96d-0', usage_metadata={'input_tokens': 24, 'output_tokens': 41, 'total_tokens': 65, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(prompt_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k3uT6eoTOl7"
      },
      "outputs": [],
      "source": [
        "# !pip install docx2txt pypdf unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-G_ZBmiSJRP",
        "outputId": "d8563a20-61c0-46e4-a84f-43751ef9c115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n",
            "Split the documents into 90 chunks.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "pdf_loader = PyPDFLoader(\"./dataset/ColPali_2407.01449v3.pdf\")\n",
        "documents = pdf_loader.load()\n",
        "\n",
        "print(len(documents))\n",
        "\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Split the documents into {len(splits)} chunks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaYyDYxKypL7",
        "outputId": "e5a3fbde-0e65-406c-9d0f-231cad1d8732"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './dataset/ColPali_2407.01449v3.pdf', 'page': 0}, page_content='ColPali: Efficient Document Retrieval with Vision Language Models\\nManuel Faysse* 1,3 Hugues Sibille∗1,4 Tony Wu∗1 Bilel Omrani1\\nGautier Viaud1 Céline Hudelot3 Pierre Colombo2,3\\n1Illuin Technology 2Equall.ai\\n3CentraleSupélec, Paris-Saclay 4ETH Zürich\\nmanuel.faysse@centralesupelec.fr\\nAbstract\\nDocuments are visually rich structures that con-\\nvey information through text, as well as tables,\\nfigures, page layouts, or fonts. While mod-\\nern document retrieval systems exhibit strong\\nperformance on query-to-text matching, they\\nstruggle to exploit visual cues efficiently, hin-\\ndering their performance on practical document\\nretrieval applications such as Retrieval Aug-\\nmented Generation. To benchmark current sys-\\ntems on visually rich document retrieval, we in-\\ntroduce the Visual Document Retrieval Bench-\\nmark ViDoRe, composed of various page-level\\nretrieving tasks spanning multiple domains,\\nlanguages, and settings. The inherent short-\\ncomings of modern systems motivate the in-\\ntroduction of a new retrieval model architec-\\nture, ColPali, which leverages the document\\nunderstanding capabilities of recent Vision Lan-\\nguage Models to produce high-quality contextu-\\nalized embeddings solely from images of doc-\\nument pages. Combined with a late interac-\\ntion matching mechanism, ColPali largely out-\\nperforms modern document retrieval pipelines\\nwhile being drastically faster and end-to-end\\ntrainable. We release all project artifacts at\\nhttps://huggingface.co/vidore.\\n1 Introduction\\nDocument Retrieval consists in matching a user\\nquery to relevant documents in a given corpus. It\\nis central to many industrial applications, either as\\na standalone ranking system (search engines) or\\nas part of more complex information extraction or\\nRetrieval Augmented Generation (RAG) pipelines.\\nOver recent years, pretrained language models have\\nenabled large improvements in text embedding\\nmodels. In practical industrial settings, however,\\nthe main performance bottleneck for efficient doc-\\nument retrieval is not in embedding model perfor-\\nmance but in the prior data ingestion pipeline. To\\n*Equal Contribution\\nFigure 1: For each term in a user query, ColPali iden-\\ntifies the most relevant document image patches (high-\\nlighted zones) and computes a query-to-page matching\\nscore. We can then swiftly retrieve the most relevant\\ndocuments from a large pre-indexed corpus.\\nindex a standard PDF document, many steps are\\nrequired. First, PDF parsers or Optical Charac-\\nter Recognition (OCR) systems are used to extract\\nwords from the pages. Document layout detec-\\ntion models can then be run to segment paragraphs,\\ntitles, and other page objects such as tables, fig-\\nures, and headers. A chunking strategy is then\\ndefined to group text passages with some seman-\\ntical coherence, and modern retrieval setups may\\neven integrate a captioning step to describe visu-\\nally rich elements in a natural language form, more\\nsuitable for embedding models. In our experiments\\n(Table 2), we typically find that optimizing the in-\\ngestion pipeline yields much greater performance\\non visually rich document retrieval than optimizing\\nthe text embedding model.\\nContribution 1: ViDoRe. In this work, we ar-\\ngue that document retrieval systems should not\\nbe evaluated solely on the capabilities of text em-\\nbedding models (Bajaj et al., 2016; Thakur et al.,\\n2021; Muennighoff et al., 2022), but should also\\n1\\narXiv:2407.01449v3  [cs.IR]  7 Oct 2024')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC5yK58oTa9-",
        "outputId": "27b792fe-6555-480f-df5d-54fd17250a89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './dataset/ColPali_2407.01449v3.pdf', 'page': 0}, page_content='mark ViDoRe, composed of various page-level\\nretrieving tasks spanning multiple domains,\\nlanguages, and settings. The inherent short-\\ncomings of modern systems motivate the in-\\ntroduction of a new retrieval model architec-\\nture, ColPali, which leverages the document\\nunderstanding capabilities of recent Vision Lan-\\nguage Models to produce high-quality contextu-\\nalized embeddings solely from images of doc-\\nument pages. Combined with a late interac-\\ntion matching mechanism, ColPali largely out-\\nperforms modern document retrieval pipelines\\nwhile being drastically faster and end-to-end\\ntrainable. We release all project artifacts at\\nhttps://huggingface.co/vidore.\\n1 Introduction\\nDocument Retrieval consists in matching a user\\nquery to relevant documents in a given corpus. It\\nis central to many industrial applications, either as\\na standalone ranking system (search engines) or\\nas part of more complex information extraction or\\nRetrieval Augmented Generation (RAG) pipelines.')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "splits[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVILJLkITgfz",
        "outputId": "9dce62d3-5976-4919-b149-80ee8beb5f9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': './dataset/ColPali_2407.01449v3.pdf', 'page': 0}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "splits[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "LAPIDXucTjFN",
        "outputId": "49523e2f-c352-44e0-d293-f725dac47a59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ColPali: Efficient Document Retrieval with Vision Language Models\\nManuel Faysse* 1,3 Hugues Sibille∗1,4 Tony Wu∗1 Bilel Omrani1\\nGautier Viaud1 Céline Hudelot3 Pierre Colombo2,3\\n1Illuin Technology 2Equall.ai\\n3CentraleSupélec, Paris-Saclay 4ETH Zürich\\nmanuel.faysse@centralesupelec.fr\\nAbstract\\nDocuments are visually rich structures that con-\\nvey information through text, as well as tables,\\nfigures, page layouts, or fonts. While mod-\\nern document retrieval systems exhibit strong\\nperformance on query-to-text matching, they\\nstruggle to exploit visual cues efficiently, hin-\\ndering their performance on practical document\\nretrieval applications such as Retrieval Aug-\\nmented Generation. To benchmark current sys-\\ntems on visually rich document retrieval, we in-\\ntroduce the Visual Document Retrieval Bench-\\nmark ViDoRe, composed of various page-level\\nretrieving tasks spanning multiple domains,\\nlanguages, and settings. The inherent short-\\ncomings of modern systems motivate the in-'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "splits[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_YJcoYjs8EU"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHPIWFXyTsGM",
        "outputId": "f0ced858-670e-449d-c62a-a7103838efb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 20 documents from the folder.\n",
            "Split the documents into 90 chunks.\n"
          ]
        }
      ],
      "source": [
        "# 1. Function to load documents from a folder\n",
        "def load_documents(folder_path: str) -> List[Document]:\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if filename.endswith('.pdf'):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif filename.endswith('.docx'):\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {filename}\")\n",
        "            continue\n",
        "        documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "# Load documents from a folder\n",
        "folder_path = \"./dataset\"\n",
        "documents = load_documents(folder_path)\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents from the folder.\")\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Split the documents into {len(splits)} chunks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRD0I2D4NSkU",
        "outputId": "0d243630-4507-4d17-9300-8244b932f225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created embeddings for 90 document chunks.\n"
          ]
        }
      ],
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# 4. Embedding Documents\n",
        "\n",
        "document_embeddings = embeddings.embed_documents([split.page_content for split in splits])\n",
        "\n",
        "print(f\"Created embeddings for {len(document_embeddings)} document chunks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t0XRI1zuTcW"
      },
      "outputs": [],
      "source": [
        "# !pip install sentence_transformers\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "document_embeddings = embedding_function.embed_documents([split.page_content for split in splits])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru-QGlOTWGvv"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain_chroma -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-m02tzkyF35"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "\n",
        "# shutil.rmtree('chroma_db')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgBuC-Xdu5gL"
      },
      "source": [
        "###Create and persist Chroma vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ov-ElhBUpJB",
        "outputId": "8fcd703f-120c-46db-ba55-2bc1423c4da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store created and persisted to './chroma_db'\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "embedding_function = OpenAIEmbeddings()\n",
        "collection_name = \"rag_service_collection_nb\"\n",
        "vectorstore = Chroma.from_documents(collection_name=collection_name, documents=splits, embedding=embedding_function, persist_directory=\"./chroma_db\")\n",
        "#db.persist()\n",
        "\n",
        "print(\"Vector store created and persisted to './chroma_db'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaRqFWA8u3I8",
        "outputId": "31309e8a-a5b0-4d65-8b36-993e8fc440af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 2 most relevant chunks for the query: 'How to understand documents visually?'\n",
            "\n",
            "Result 1:\n",
            "Source: ./dataset/ColPali_2407.01449v3.pdf\n",
            "Content: index a standard PDF document, many steps are\n",
            "required. First, PDF parsers or Optical Charac-\n",
            "ter Recognition (OCR) systems are used to extract\n",
            "words from the pages. Document layout detec-\n",
            "tion models can then be run to segment paragraphs,\n",
            "titles, and other page objects such as tables, fig-\n",
            "ures, and headers. A chunking strategy is then\n",
            "defined to group text passages with some seman-\n",
            "tical coherence, and modern retrieval setups may\n",
            "even integrate a captioning step to describe visu-\n",
            "ally rich elements in a natural language form, more\n",
            "suitable for embedding models. In our experiments\n",
            "(Table 2), we typically find that optimizing the in-\n",
            "gestion pipeline yields much greater performance\n",
            "on visually rich document retrieval than optimizing\n",
            "the text embedding model.\n",
            "Contribution 1: ViDoRe. In this work, we ar-\n",
            "gue that document retrieval systems should not\n",
            "be evaluated solely on the capabilities of text em-\n",
            "bedding models (Bajaj et al., 2016; Thakur et al.,\n",
            "\n",
            "Result 2:\n",
            "Source: ./dataset/ColPali_2407.01449v3.pdf\n",
            "Content: ColPali: Efficient Document Retrieval with Vision Language Models\n",
            "Manuel Faysse* 1,3 Hugues Sibille∗1,4 Tony Wu∗1 Bilel Omrani1\n",
            "Gautier Viaud1 Céline Hudelot3 Pierre Colombo2,3\n",
            "1Illuin Technology 2Equall.ai\n",
            "3CentraleSupélec, Paris-Saclay 4ETH Zürich\n",
            "manuel.faysse@centralesupelec.fr\n",
            "Abstract\n",
            "Documents are visually rich structures that con-\n",
            "vey information through text, as well as tables,\n",
            "figures, page layouts, or fonts. While mod-\n",
            "ern document retrieval systems exhibit strong\n",
            "performance on query-to-text matching, they\n",
            "struggle to exploit visual cues efficiently, hin-\n",
            "dering their performance on practical document\n",
            "retrieval applications such as Retrieval Aug-\n",
            "mented Generation. To benchmark current sys-\n",
            "tems on visually rich document retrieval, we in-\n",
            "troduce the Visual Document Retrieval Bench-\n",
            "mark ViDoRe, composed of various page-level\n",
            "retrieving tasks spanning multiple domains,\n",
            "languages, and settings. The inherent short-\n",
            "comings of modern systems motivate the in-\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5. Perform similarity search\n",
        "\n",
        "query = \"How to understand documents visually?\"\n",
        "search_results = vectorstore.similarity_search(query, k=5)\n",
        "\n",
        "print(f\"\\nTop 2 most relevant chunks for the query: '{query}'\\n\")\n",
        "for i, result in enumerate(search_results, 1):\n",
        "    print(f\"Result {i}:\")\n",
        "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
        "    print(f\"Content: {result.page_content}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGkvzzIZvC2R",
        "outputId": "081f1019-f8a3-4dfa-9ed4-22f2d7198cfa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 0, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='index a standard PDF document, many steps are\\nrequired. First, PDF parsers or Optical Charac-\\nter Recognition (OCR) systems are used to extract\\nwords from the pages. Document layout detec-\\ntion models can then be run to segment paragraphs,\\ntitles, and other page objects such as tables, fig-\\nures, and headers. A chunking strategy is then\\ndefined to group text passages with some seman-\\ntical coherence, and modern retrieval setups may\\neven integrate a captioning step to describe visu-\\nally rich elements in a natural language form, more\\nsuitable for embedding models. In our experiments\\n(Table 2), we typically find that optimizing the in-\\ngestion pipeline yields much greater performance\\non visually rich document retrieval than optimizing\\nthe text embedding model.\\nContribution 1: ViDoRe. In this work, we ar-\\ngue that document retrieval systems should not\\nbe evaluated solely on the capabilities of text em-\\nbedding models (Bajaj et al., 2016; Thakur et al.,'),\n",
              " Document(metadata={'page': 0, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='ColPali: Efficient Document Retrieval with Vision Language Models\\nManuel Faysse* 1,3 Hugues Sibille∗1,4 Tony Wu∗1 Bilel Omrani1\\nGautier Viaud1 Céline Hudelot3 Pierre Colombo2,3\\n1Illuin Technology 2Equall.ai\\n3CentraleSupélec, Paris-Saclay 4ETH Zürich\\nmanuel.faysse@centralesupelec.fr\\nAbstract\\nDocuments are visually rich structures that con-\\nvey information through text, as well as tables,\\nfigures, page layouts, or fonts. While mod-\\nern document retrieval systems exhibit strong\\nperformance on query-to-text matching, they\\nstruggle to exploit visual cues efficiently, hin-\\ndering their performance on practical document\\nretrieval applications such as Retrieval Aug-\\nmented Generation. To benchmark current sys-\\ntems on visually rich document retrieval, we in-\\ntroduce the Visual Document Retrieval Bench-\\nmark ViDoRe, composed of various page-level\\nretrieving tasks spanning multiple domains,\\nlanguages, and settings. The inherent short-\\ncomings of modern systems motivate the in-')]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XRiNA4y8vkcz"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYCoxqbcvuht",
        "outputId": "aed01866-ba79-4685-cc45-9a060686983d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content=\"Answer the question based only on the following context:\\n[Document(metadata={'page': 0, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='index a standard PDF document, many steps are\\\\nrequired. First, PDF parsers or Optical Charac-\\\\nter Recognition (OCR) systems are used to extract\\\\nwords from the pages. Document layout detec-\\\\ntion models can then be run to segment paragraphs,\\\\ntitles, and other page objects such as tables, fig-\\\\nures, and headers. A chunking strategy is then\\\\ndefined to group text passages with some seman-\\\\ntical coherence, and modern retrieval setups may\\\\neven integrate a captioning step to describe visu-\\\\nally rich elements in a natural language form, more\\\\nsuitable for embedding models. In our experiments\\\\n(Table 2), we typically find that optimizing the in-\\\\ngestion pipeline yields much greater performance\\\\non visually rich document retrieval than optimizing\\\\nthe text embedding model.\\\\nContribution 1: ViDoRe. In this work, we ar-\\\\ngue that document retrieval systems should not\\\\nbe evaluated solely on the capabilities of text em-\\\\nbedding models (Bajaj et al., 2016; Thakur et al.,'), Document(metadata={'page': 0, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='ColPali: Efficient Document Retrieval with Vision Language Models\\\\nManuel Faysse* 1,3 Hugues Sibille∗1,4 Tony Wu∗1 Bilel Omrani1\\\\nGautier Viaud1 Céline Hudelot3 Pierre Colombo2,3\\\\n1Illuin Technology 2Equall.ai\\\\n3CentraleSupélec, Paris-Saclay 4ETH Zürich\\\\nmanuel.faysse@centralesupelec.fr\\\\nAbstract\\\\nDocuments are visually rich structures that con-\\\\nvey information through text, as well as tables,\\\\nfigures, page layouts, or fonts. While mod-\\\\nern document retrieval systems exhibit strong\\\\nperformance on query-to-text matching, they\\\\nstruggle to exploit visual cues efficiently, hin-\\\\ndering their performance on practical document\\\\nretrieval applications such as Retrieval Aug-\\\\nmented Generation. To benchmark current sys-\\\\ntems on visually rich document retrieval, we in-\\\\ntroduce the Visual Document Retrieval Bench-\\\\nmark ViDoRe, composed of various page-level\\\\nretrieving tasks spanning multiple domains,\\\\nlanguages, and settings. The inherent short-\\\\ncomings of modern systems motivate the in-')]\\n\\nQuestion: How to understand documents visually?\\n\\nAnswer: \", additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt\n",
        ")\n",
        "rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "uztRXXwDvud9"
      },
      "outputs": [],
      "source": [
        "def docs2str(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk-VtijcwZw9",
        "outputId": "b976c90d-5d8e-427b-cee7-2281167b746c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='Answer the question based only on the following context:\\nindex a standard PDF document, many steps are\\nrequired. First, PDF parsers or Optical Charac-\\nter Recognition (OCR) systems are used to extract\\nwords from the pages. Document layout detec-\\ntion models can then be run to segment paragraphs,\\ntitles, and other page objects such as tables, fig-\\nures, and headers. A chunking strategy is then\\ndefined to group text passages with some seman-\\ntical coherence, and modern retrieval setups may\\neven integrate a captioning step to describe visu-\\nally rich elements in a natural language form, more\\nsuitable for embedding models. In our experiments\\n(Table 2), we typically find that optimizing the in-\\ngestion pipeline yields much greater performance\\non visually rich document retrieval than optimizing\\nthe text embedding model.\\nContribution 1: ViDoRe. In this work, we ar-\\ngue that document retrieval systems should not\\nbe evaluated solely on the capabilities of text em-\\nbedding models (Bajaj et al., 2016; Thakur et al.,\\n\\nColPali: Efficient Document Retrieval with Vision Language Models\\nManuel Faysse* 1,3 Hugues Sibille∗1,4 Tony Wu∗1 Bilel Omrani1\\nGautier Viaud1 Céline Hudelot3 Pierre Colombo2,3\\n1Illuin Technology 2Equall.ai\\n3CentraleSupélec, Paris-Saclay 4ETH Zürich\\nmanuel.faysse@centralesupelec.fr\\nAbstract\\nDocuments are visually rich structures that con-\\nvey information through text, as well as tables,\\nfigures, page layouts, or fonts. While mod-\\nern document retrieval systems exhibit strong\\nperformance on query-to-text matching, they\\nstruggle to exploit visual cues efficiently, hin-\\ndering their performance on practical document\\nretrieval applications such as Retrieval Aug-\\nmented Generation. To benchmark current sys-\\ntems on visually rich document retrieval, we in-\\ntroduce the Visual Document Retrieval Bench-\\nmark ViDoRe, composed of various page-level\\nretrieving tasks spanning multiple domains,\\nlanguages, and settings. The inherent short-\\ncomings of modern systems motivate the in-\\n\\nQuestion: How to understand documents visually?\\n\\nAnswer: ', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()} | prompt\n",
        ")\n",
        "rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gjDAr3lwbXA",
        "outputId": "7a9df563-e772-401a-be4f-7abecb58ce40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To understand documents visually, one can utilize several steps that involve extracting and analyzing visual elements. Initially, PDF parsers or Optical Character Recognition (OCR) systems are employed to extract text from the document pages. Following this, document layout detection models are used to identify and segment various components such as paragraphs, titles, tables, figures, and headers. A chunking strategy is then implemented to organize text passages with semantic coherence. Additionally, integrating a captioning step can help describe visually rich elements in a natural language format, making it more suitable for embedding models. This multi-step approach allows for a comprehensive understanding of the visual structure and content of documents.\n"
          ]
        }
      ],
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "question = query\n",
        "response = rag_chain.invoke(question)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RPDMn9q4GbE"
      },
      "source": [
        "###Conversational RAG\n",
        "\n",
        "####Handling Follow Up Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0OW5ak-Bqf1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbbjrk1cwl19"
      },
      "outputs": [],
      "source": [
        "# Example conversation\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "chat_history = []\n",
        "chat_history.extend([\n",
        "    HumanMessage(content=question),\n",
        "    AIMessage(content=response)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApN5nctt2XSn",
        "outputId": "61d19ef0-2efb-44df-f098-1754cb19be2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='When was GreenGrow Innovations founded?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='GreenGrow Innovations was founded in 2010.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Sm2ju8n72YXY",
        "outputId": "eebba764-fee0-412a-dafc-528d5cb49d6f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Where is GreenGrow Innovations headquartered?'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# history_aware_retriever = create_history_aware_retriever(\n",
        "#     llm, retriever, contextualize_q_prompt\n",
        "# )\n",
        "contextualize_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
        "contextualize_chain.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\": chat_history})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vShrBX-M24_r",
        "outputId": "77755e1c-890c-49e8-deb0-daa9de3ae6fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content=\"The company's breakthrough came in 2018 with the introduction of the EcoHarvest System, an integrated solution that combined smart irrigation, soil monitoring, and automated harvesting techniques. This system caught the attention of large-scale farmers across the United States, propelling GreenGrow to national prominence.\\n\\n\\n\\nToday, GreenGrow Innovations employs over 200 people and has expanded its operations to include offices in California and Iowa. The company continues to focus on developing sustainable agricultural technologies, with ongoing projects in vertical farming, drought-resistant crop development, and AI-powered farm management systems.\\n\\n\\n\\nDespite its growth, GreenGrow remains committed to its original mission of promoting sustainable farming practices. The company regularly partners with universities and research institutions to advance the field of agricultural technology and hosts annual conferences to share knowledge with farmers and other industry professionals.\"),\n",
              " Document(metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='GreenGrow Innovations was founded in 2010 by Sarah Chen and Michael Rodriguez, two agricultural engineers with a passion for sustainable farming. The company started in a small garage in Portland, Oregon, with a simple mission: to make farming more environmentally friendly and efficient.\\n\\n\\n\\nIn its early days, GreenGrow focused on developing smart irrigation systems that could significantly reduce water usage in agriculture. Their first product, the WaterWise Sensor, was launched in 2012 and quickly gained popularity among local farmers. This success allowed the company to expand its research and development efforts.\\n\\n\\n\\nBy 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research facility in the outskirts of Portland. This move coincided with the development of their second major product, the SoilHealth Monitor, which used advanced sensors to analyze soil composition and provide real-time recommendations for optimal crop growth.')]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "history_aware_retriever.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\": chat_history})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0BMwLBo7jhV",
        "outputId": "801db6d2-0cf6-448d-81ae-bfd90ce9d479"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/docs/Company_ QuantumNext Systems.docx'}, page_content='Company: QuantumNext Systems\\n\\nHeadquarters: QuantumNext Systems is headquartered in Bangalore, Karnataka, India. The company, specializing in quantum computing and advanced data processing, is situated in the bustling tech metropolis of Bangalore, often referred to as the \"Silicon Valley of India.\" From this technology capital, QuantumNext Systems is well-positioned to tap into India\\'s rich pool of engineering talent and growing tech ecosystem, enabling it to push the boundaries of computational innovation.'),\n",
              " Document(metadata={'source': '/content/docs/Company_ TechWave Innovations.docx'}, page_content='Company: TechWave Innovations\\n\\nHeadquarters: TechWave Innovations is headquartered in San Francisco, California, USA. As a leader in cutting-edge AI and machine learning solutions, the company thrives in the heart of Silicon Valley, benefiting from its proximity to tech giants and a dynamic startup ecosystem. With its headquarters in this global technology hub, TechWave Innovations has access to top talent and a vast network of innovation-driven enterprises.')]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke(\"Where it is headquartered?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AYJ1jce6cbQ"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.\"),\n",
        "    #  (\"system\", \"Tell me joke on Programming\"),\n",
        "    (\"system\", \"Context: {context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkeEE0hD65zW",
        "outputId": "accc91c8-5db0-418f-fdec-114ae6f7f47c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'Where it is headquartered?',\n",
              " 'chat_history': [HumanMessage(content='When was GreenGrow Innovations founded?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='GreenGrow Innovations was founded in 2010.', additional_kwargs={}, response_metadata={})],\n",
              " 'context': [Document(metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content=\"The company's breakthrough came in 2018 with the introduction of the EcoHarvest System, an integrated solution that combined smart irrigation, soil monitoring, and automated harvesting techniques. This system caught the attention of large-scale farmers across the United States, propelling GreenGrow to national prominence.\\n\\n\\n\\nToday, GreenGrow Innovations employs over 200 people and has expanded its operations to include offices in California and Iowa. The company continues to focus on developing sustainable agricultural technologies, with ongoing projects in vertical farming, drought-resistant crop development, and AI-powered farm management systems.\\n\\n\\n\\nDespite its growth, GreenGrow remains committed to its original mission of promoting sustainable farming practices. The company regularly partners with universities and research institutions to advance the field of agricultural technology and hosts annual conferences to share knowledge with farmers and other industry professionals.\"),\n",
              "  Document(metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='GreenGrow Innovations was founded in 2010 by Sarah Chen and Michael Rodriguez, two agricultural engineers with a passion for sustainable farming. The company started in a small garage in Portland, Oregon, with a simple mission: to make farming more environmentally friendly and efficient.\\n\\n\\n\\nIn its early days, GreenGrow focused on developing smart irrigation systems that could significantly reduce water usage in agriculture. Their first product, the WaterWise Sensor, was launched in 2012 and quickly gained popularity among local farmers. This success allowed the company to expand its research and development efforts.\\n\\n\\n\\nBy 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research facility in the outskirts of Portland. This move coincided with the development of their second major product, the SoilHealth Monitor, which used advanced sensors to analyze soil composition and provide real-time recommendations for optimal crop growth.')],\n",
              " 'answer': 'GreenGrow Innovations is headquartered in Portland, Oregon.'}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\":chat_history})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qdzx68u5aCv"
      },
      "source": [
        "###Building Multi User Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWufJa-c5HaC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftIORKEF3coG"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "DB_NAME = \"rag_app.db\"\n",
        "\n",
        "def get_db_connection():\n",
        "    conn = sqlite3.connect(DB_NAME)\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    return conn\n",
        "\n",
        "def create_application_logs():\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
        "                    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                     session_id TEXT,\n",
        "                     user_query TEXT,\n",
        "                     gpt_response TEXT,\n",
        "                     model TEXT,\n",
        "                     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
        "    conn.close()\n",
        "\n",
        "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
        "                 (session_id, user_query, gpt_response, model))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def get_chat_history(session_id):\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
        "    messages = []\n",
        "    for row in cursor.fetchall():\n",
        "        messages.extend([\n",
        "            {\"role\": \"human\", \"content\": row['user_query']},\n",
        "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
        "        ])\n",
        "    conn.close()\n",
        "    return messages\n",
        "\n",
        "# Initialize the database\n",
        "create_application_logs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M--sjKYN6JIQ",
        "outputId": "1461b818-15d8-4f67-c6ac-4a10b0f7a5d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "Human: When was GreenGrow Innovations founded?\n",
            "AI: GreenGrow Innovations was founded in 2010.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "session_id = str(uuid.uuid4())\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "question1 = \"When was GreenGrow Innovations founded?\"\n",
        "answer1 = rag_chain.invoke({\"input\": question1, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question1, answer1, \"gpt-4-o-mini\")\n",
        "print(f\"Human: {question1}\")\n",
        "print(f\"AI: {answer1}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqDRm5Rl6P5U",
        "outputId": "39944daf-182e-4148-85dd-83ac766e637e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'human', 'content': 'When was GreenGrow Innovations founded?'}, {'role': 'ai', 'content': 'GreenGrow Innovations was founded in 2010.'}]\n",
            "Human: Where it is headquartered?\n",
            "AI: GreenGrow Innovations is headquartered in Portland, Oregon. Additionally, the company has expanded its operations to include offices in California and Iowa.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question2 = \"Where it is headquartered?\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question2, answer2, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question2}\")\n",
        "print(f\"AI: {answer2}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgQiUzN68KYl"
      },
      "source": [
        "New User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jWmJ0FgD78AO"
      },
      "outputs": [],
      "source": [
        "session_id = str(uuid.uuid4())\n",
        "question = \"What is GreenGrow\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer = rag_chain.invoke({\"input\": question, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question, answer, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question}\")\n",
        "print(f\"AI: {answer}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ssvjqblJ8SC3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
