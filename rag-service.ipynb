{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import IFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvZGjmr9Rl6w",
        "outputId": "b4f1bae5-87e8-43b6-b648-0473d8738bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3.13\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ynuxRM7_-Nhu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkGT7MDh-mGk"
      },
      "source": [
        "###Call LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-G_ZBmiSJRP",
        "outputId": "d8563a20-61c0-46e4-a84f-43751ef9c115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n",
            "Split the documents into 142 chunks.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=32,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "pdf_loader = PyPDFLoader(\"./dataset/ColPali_2407.01449v3.pdf\")\n",
        "documents = pdf_loader.load()\n",
        "\n",
        "print(len(documents))\n",
        "\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Split the documents into {len(splits)} chunks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHPIWFXyTsGM",
        "outputId": "f0ced858-670e-449d-c62a-a7103838efb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 20 documents from the folder.\n",
            "Split the documents into 142 chunks.\n"
          ]
        }
      ],
      "source": [
        "# 1. Function to load documents from a folder\n",
        "def load_documents(folder_path: str) -> List[Document]:\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if filename.endswith('.pdf'):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif filename.endswith('.docx'):\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {filename}\")\n",
        "            continue\n",
        "        documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "# Load documents from a folder\n",
        "folder_path = \"./dataset\"\n",
        "documents = load_documents(folder_path)\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents from the folder.\")\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Split the documents into {len(splits)} chunks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRD0I2D4NSkU",
        "outputId": "0d243630-4507-4d17-9300-8244b932f225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Created embeddings for 90 document chunks using text-embedding-ada-002 model\n"
          ]
        }
      ],
      "source": [
        "# embeddings = OpenAIEmbeddings()\n",
        "# document_embeddings = embeddings.embed_documents([split.page_content for split in splits])\n",
        "# print(f\"Created embeddings for {len(document_embeddings)} document chunks using {embeddings.model} model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6t0XRI1zuTcW"
      },
      "outputs": [],
      "source": [
        "# # from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "# from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "# # embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "# document_embeddings = embedding_function.embed_documents([split.page_content for split in splits])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgBuC-Xdu5gL"
      },
      "source": [
        "###Create and persist Chroma vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ov-ElhBUpJB",
        "outputId": "8fcd703f-120c-46db-ba55-2bc1423c4da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store created and persisted to './chroma_db'\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "embedding_function = OpenAIEmbeddings()\n",
        "collection_name = \"rag_service_collection_nb\"\n",
        "vectorstore = Chroma.from_documents(collection_name=collection_name, documents=splits, embedding=embedding_function, persist_directory=\"./chroma_db\")\n",
        "#db.persist()\n",
        "\n",
        "print(\"Vector store created and persisted to './chroma_db'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"How to understand documents visually?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaRqFWA8u3I8",
        "outputId": "31309e8a-a5b0-4d65-8b36-993e8fc440af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 2 most relevant chunks for the query: 'How to understand documents visually?'\n",
            "\n",
            "Result 1:\n",
            "Source: ./dataset/ColPali_2407.01449v3.pdf\n",
            "Content: Visually Rich Document Understanding. To\n",
            "go beyond text, some document-focused models\n",
            "jointly encode text tokens alongside visual or docu-\n",
            "ment layout features (Appalaraju et al., 2021; Kim\n",
            "et al., 2021; Huang et al., 2022; Tang et al., 2022).\n",
            "\n",
            "Result 2:\n",
            "Source: ./dataset/ColPali_2407.01449v3.pdf\n",
            "Content: manuel.faysse@centralesupelec.fr\n",
            "Abstract\n",
            "Documents are visually rich structures that con-\n",
            "vey information through text, as well as tables,\n",
            "figures, page layouts, or fonts. While mod-\n",
            "ern document retrieval systems exhibit strong\n",
            "\n",
            "Result 3:\n",
            "Source: ./dataset/ColPali_2407.01449v3.pdf\n",
            "Content: consider the context and visual elements of the doc-\n",
            "uments to be retrieved. To this end, we create and\n",
            "openly release ViDoRe, a comprehensive bench-\n",
            "mark to evaluate systems on page-level document\n",
            "retrieval with a wide coverage of domains, visual\n",
            "\n",
            "Result 4:\n",
            "Source: ./dataset/ColPali_2407.01449v3.pdf\n",
            "Content: rely on visual elements to more efficiently convey\n",
            "information to human readers, text-only systems\n",
            "barely tap into these visual cues.\n",
            "To our knowledge, no benchmark evaluates docu-\n",
            "ment retrieval methods by considering both textual\n",
            "\n",
            "Result 5:\n",
            "Source: ./dataset/ColPali_2407.01449v3.pdf\n",
            "Content: documents from a large pre-indexed corpus.\n",
            "index a standard PDF document, many steps are\n",
            "required. First, PDF parsers or Optical Charac-\n",
            "ter Recognition (OCR) systems are used to extract\n",
            "words from the pages. Document layout detec-\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5. Perform similarity search\n",
        "search_results = vectorstore.similarity_search(query, k=5)\n",
        "\n",
        "print(f\"\\nTop 2 most relevant chunks for the query: '{query}'\\n\")\n",
        "for i, result in enumerate(search_results, 1):\n",
        "    print(f\"Result {i}:\")\n",
        "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
        "    print(f\"Content: {result.page_content}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGkvzzIZvC2R",
        "outputId": "081f1019-f8a3-4dfa-9ed4-22f2d7198cfa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 2, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='Visually Rich Document Understanding. To\\ngo beyond text, some document-focused models\\njointly encode text tokens alongside visual or docu-\\nment layout features (Appalaraju et al., 2021; Kim\\net al., 2021; Huang et al., 2022; Tang et al., 2022).'),\n",
              " Document(metadata={'page': 0, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='manuel.faysse@centralesupelec.fr\\nAbstract\\nDocuments are visually rich structures that con-\\nvey information through text, as well as tables,\\nfigures, page layouts, or fonts. While mod-\\nern document retrieval systems exhibit strong'),\n",
              " Document(metadata={'page': 1, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='consider the context and visual elements of the doc-\\numents to be retrieved. To this end, we create and\\nopenly release ViDoRe, a comprehensive bench-\\nmark to evaluate systems on page-level document\\nretrieval with a wide coverage of domains, visual'),\n",
              " Document(metadata={'page': 2, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='rely on visual elements to more efficiently convey\\ninformation to human readers, text-only systems\\nbarely tap into these visual cues.\\nTo our knowledge, no benchmark evaluates docu-\\nment retrieval methods by considering both textual'),\n",
              " Document(metadata={'page': 0, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='documents from a large pre-indexed corpus.\\nindex a standard PDF document, many steps are\\nrequired. First, PDF parsers or Optical Charac-\\nter Recognition (OCR) systems are used to extract\\nwords from the pages. Document layout detec-')]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "XRiNA4y8vkcz"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYCoxqbcvuht",
        "outputId": "aed01866-ba79-4685-cc45-9a060686983d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content=\"Answer the question based only on the following context:\\n[Document(metadata={'page': 2, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='Visually Rich Document Understanding. To\\\\ngo beyond text, some document-focused models\\\\njointly encode text tokens alongside visual or docu-\\\\nment layout features (Appalaraju et al., 2021; Kim\\\\net al., 2021; Huang et al., 2022; Tang et al., 2022).'), Document(metadata={'page': 0, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='manuel.faysse@centralesupelec.fr\\\\nAbstract\\\\nDocuments are visually rich structures that con-\\\\nvey information through text, as well as tables,\\\\nfigures, page layouts, or fonts. While mod-\\\\nern document retrieval systems exhibit strong'), Document(metadata={'page': 1, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='consider the context and visual elements of the doc-\\\\numents to be retrieved. To this end, we create and\\\\nopenly release ViDoRe, a comprehensive bench-\\\\nmark to evaluate systems on page-level document\\\\nretrieval with a wide coverage of domains, visual'), Document(metadata={'page': 2, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='rely on visual elements to more efficiently convey\\\\ninformation to human readers, text-only systems\\\\nbarely tap into these visual cues.\\\\nTo our knowledge, no benchmark evaluates docu-\\\\nment retrieval methods by considering both textual'), Document(metadata={'page': 0, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='documents from a large pre-indexed corpus.\\\\nindex a standard PDF document, many steps are\\\\nrequired. First, PDF parsers or Optical Charac-\\\\nter Recognition (OCR) systems are used to extract\\\\nwords from the pages. Document layout detec-')]\\n\\nQuestion: How to understand documents visually?\\n\\nAnswer: \", additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt\n",
        ")\n",
        "rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "uztRXXwDvud9"
      },
      "outputs": [],
      "source": [
        "def docs2str(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk-VtijcwZw9",
        "outputId": "b976c90d-5d8e-427b-cee7-2281167b746c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='Answer the question based only on the following context:\\nVisually Rich Document Understanding. To\\ngo beyond text, some document-focused models\\njointly encode text tokens alongside visual or docu-\\nment layout features (Appalaraju et al., 2021; Kim\\net al., 2021; Huang et al., 2022; Tang et al., 2022).\\n\\nmanuel.faysse@centralesupelec.fr\\nAbstract\\nDocuments are visually rich structures that con-\\nvey information through text, as well as tables,\\nfigures, page layouts, or fonts. While mod-\\nern document retrieval systems exhibit strong\\n\\nconsider the context and visual elements of the doc-\\numents to be retrieved. To this end, we create and\\nopenly release ViDoRe, a comprehensive bench-\\nmark to evaluate systems on page-level document\\nretrieval with a wide coverage of domains, visual\\n\\nrely on visual elements to more efficiently convey\\ninformation to human readers, text-only systems\\nbarely tap into these visual cues.\\nTo our knowledge, no benchmark evaluates docu-\\nment retrieval methods by considering both textual\\n\\ndocuments from a large pre-indexed corpus.\\nindex a standard PDF document, many steps are\\nrequired. First, PDF parsers or Optical Charac-\\nter Recognition (OCR) systems are used to extract\\nwords from the pages. Document layout detec-\\n\\nQuestion: How to understand documents visually?\\n\\nAnswer: ', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()} | prompt\n",
        ")\n",
        "rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gjDAr3lwbXA",
        "outputId": "7a9df563-e772-401a-be4f-7abecb58ce40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To understand documents visually, one should consider the various visual elements that contribute to the overall information conveyed, such as tables, figures, page layouts, and font styles. Document-focused models can aid in this process by jointly encoding text tokens alongside these visual features. By leveraging these visual cues, one can gain a more comprehensive understanding of the document's content, as traditional text-only systems often overlook the significance of these visual components.\n"
          ]
        }
      ],
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "question = query\n",
        "response = rag_chain.invoke(question)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RPDMn9q4GbE"
      },
      "source": [
        "###Conversational RAG\n",
        "\n",
        "####Handling Follow Up Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0OW5ak-Bqf1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbbjrk1cwl19"
      },
      "outputs": [],
      "source": [
        "# Example conversation\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "chat_history = []\n",
        "chat_history.extend([\n",
        "    HumanMessage(content=question),\n",
        "    AIMessage(content=response)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApN5nctt2XSn",
        "outputId": "61d19ef0-2efb-44df-f098-1754cb19be2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='When was GreenGrow Innovations founded?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='GreenGrow Innovations was founded in 2010.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Sm2ju8n72YXY",
        "outputId": "eebba764-fee0-412a-dafc-528d5cb49d6f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Where is GreenGrow Innovations headquartered?'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# history_aware_retriever = create_history_aware_retriever(\n",
        "#     llm, retriever, contextualize_q_prompt\n",
        "# )\n",
        "contextualize_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
        "contextualize_chain.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\": chat_history})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vShrBX-M24_r",
        "outputId": "77755e1c-890c-49e8-deb0-daa9de3ae6fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content=\"The company's breakthrough came in 2018 with the introduction of the EcoHarvest System, an integrated solution that combined smart irrigation, soil monitoring, and automated harvesting techniques. This system caught the attention of large-scale farmers across the United States, propelling GreenGrow to national prominence.\\n\\n\\n\\nToday, GreenGrow Innovations employs over 200 people and has expanded its operations to include offices in California and Iowa. The company continues to focus on developing sustainable agricultural technologies, with ongoing projects in vertical farming, drought-resistant crop development, and AI-powered farm management systems.\\n\\n\\n\\nDespite its growth, GreenGrow remains committed to its original mission of promoting sustainable farming practices. The company regularly partners with universities and research institutions to advance the field of agricultural technology and hosts annual conferences to share knowledge with farmers and other industry professionals.\"),\n",
              " Document(metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='GreenGrow Innovations was founded in 2010 by Sarah Chen and Michael Rodriguez, two agricultural engineers with a passion for sustainable farming. The company started in a small garage in Portland, Oregon, with a simple mission: to make farming more environmentally friendly and efficient.\\n\\n\\n\\nIn its early days, GreenGrow focused on developing smart irrigation systems that could significantly reduce water usage in agriculture. Their first product, the WaterWise Sensor, was launched in 2012 and quickly gained popularity among local farmers. This success allowed the company to expand its research and development efforts.\\n\\n\\n\\nBy 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research facility in the outskirts of Portland. This move coincided with the development of their second major product, the SoilHealth Monitor, which used advanced sensors to analyze soil composition and provide real-time recommendations for optimal crop growth.')]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "history_aware_retriever.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\": chat_history})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0BMwLBo7jhV",
        "outputId": "801db6d2-0cf6-448d-81ae-bfd90ce9d479"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/docs/Company_ QuantumNext Systems.docx'}, page_content='Company: QuantumNext Systems\\n\\nHeadquarters: QuantumNext Systems is headquartered in Bangalore, Karnataka, India. The company, specializing in quantum computing and advanced data processing, is situated in the bustling tech metropolis of Bangalore, often referred to as the \"Silicon Valley of India.\" From this technology capital, QuantumNext Systems is well-positioned to tap into India\\'s rich pool of engineering talent and growing tech ecosystem, enabling it to push the boundaries of computational innovation.'),\n",
              " Document(metadata={'source': '/content/docs/Company_ TechWave Innovations.docx'}, page_content='Company: TechWave Innovations\\n\\nHeadquarters: TechWave Innovations is headquartered in San Francisco, California, USA. As a leader in cutting-edge AI and machine learning solutions, the company thrives in the heart of Silicon Valley, benefiting from its proximity to tech giants and a dynamic startup ecosystem. With its headquarters in this global technology hub, TechWave Innovations has access to top talent and a vast network of innovation-driven enterprises.')]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke(\"Where it is headquartered?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AYJ1jce6cbQ"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.\"),\n",
        "    #  (\"system\", \"Tell me joke on Programming\"),\n",
        "    (\"system\", \"Context: {context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkeEE0hD65zW",
        "outputId": "accc91c8-5db0-418f-fdec-114ae6f7f47c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'Where it is headquartered?',\n",
              " 'chat_history': [HumanMessage(content='When was GreenGrow Innovations founded?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='GreenGrow Innovations was founded in 2010.', additional_kwargs={}, response_metadata={})],\n",
              " 'context': [Document(metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content=\"The company's breakthrough came in 2018 with the introduction of the EcoHarvest System, an integrated solution that combined smart irrigation, soil monitoring, and automated harvesting techniques. This system caught the attention of large-scale farmers across the United States, propelling GreenGrow to national prominence.\\n\\n\\n\\nToday, GreenGrow Innovations employs over 200 people and has expanded its operations to include offices in California and Iowa. The company continues to focus on developing sustainable agricultural technologies, with ongoing projects in vertical farming, drought-resistant crop development, and AI-powered farm management systems.\\n\\n\\n\\nDespite its growth, GreenGrow remains committed to its original mission of promoting sustainable farming practices. The company regularly partners with universities and research institutions to advance the field of agricultural technology and hosts annual conferences to share knowledge with farmers and other industry professionals.\"),\n",
              "  Document(metadata={'source': '/content/docs/GreenGrow Innovations_ Company History.docx'}, page_content='GreenGrow Innovations was founded in 2010 by Sarah Chen and Michael Rodriguez, two agricultural engineers with a passion for sustainable farming. The company started in a small garage in Portland, Oregon, with a simple mission: to make farming more environmentally friendly and efficient.\\n\\n\\n\\nIn its early days, GreenGrow focused on developing smart irrigation systems that could significantly reduce water usage in agriculture. Their first product, the WaterWise Sensor, was launched in 2012 and quickly gained popularity among local farmers. This success allowed the company to expand its research and development efforts.\\n\\n\\n\\nBy 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research facility in the outskirts of Portland. This move coincided with the development of their second major product, the SoilHealth Monitor, which used advanced sensors to analyze soil composition and provide real-time recommendations for optimal crop growth.')],\n",
              " 'answer': 'GreenGrow Innovations is headquartered in Portland, Oregon.'}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\":chat_history})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qdzx68u5aCv"
      },
      "source": [
        "###Building Multi User Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWufJa-c5HaC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftIORKEF3coG"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "DB_NAME = \"rag_app.db\"\n",
        "\n",
        "def get_db_connection():\n",
        "    conn = sqlite3.connect(DB_NAME)\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    return conn\n",
        "\n",
        "def create_application_logs():\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
        "                    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                     session_id TEXT,\n",
        "                     user_query TEXT,\n",
        "                     gpt_response TEXT,\n",
        "                     model TEXT,\n",
        "                     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
        "    conn.close()\n",
        "\n",
        "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
        "                 (session_id, user_query, gpt_response, model))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def get_chat_history(session_id):\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
        "    messages = []\n",
        "    for row in cursor.fetchall():\n",
        "        messages.extend([\n",
        "            {\"role\": \"human\", \"content\": row['user_query']},\n",
        "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
        "        ])\n",
        "    conn.close()\n",
        "    return messages\n",
        "\n",
        "# Initialize the database\n",
        "create_application_logs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M--sjKYN6JIQ",
        "outputId": "1461b818-15d8-4f67-c6ac-4a10b0f7a5d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "Human: When was GreenGrow Innovations founded?\n",
            "AI: GreenGrow Innovations was founded in 2010.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "session_id = str(uuid.uuid4())\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "question1 = \"When was GreenGrow Innovations founded?\"\n",
        "answer1 = rag_chain.invoke({\"input\": question1, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question1, answer1, \"gpt-4-o-mini\")\n",
        "print(f\"Human: {question1}\")\n",
        "print(f\"AI: {answer1}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqDRm5Rl6P5U",
        "outputId": "39944daf-182e-4148-85dd-83ac766e637e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'human', 'content': 'When was GreenGrow Innovations founded?'}, {'role': 'ai', 'content': 'GreenGrow Innovations was founded in 2010.'}]\n",
            "Human: Where it is headquartered?\n",
            "AI: GreenGrow Innovations is headquartered in Portland, Oregon. Additionally, the company has expanded its operations to include offices in California and Iowa.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question2 = \"Where it is headquartered?\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question2, answer2, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question2}\")\n",
        "print(f\"AI: {answer2}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgQiUzN68KYl"
      },
      "source": [
        "New User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jWmJ0FgD78AO"
      },
      "outputs": [],
      "source": [
        "session_id = str(uuid.uuid4())\n",
        "question = \"What is GreenGrow\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer = rag_chain.invoke({\"input\": question, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question, answer, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question}\")\n",
        "print(f\"AI: {answer}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ssvjqblJ8SC3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
