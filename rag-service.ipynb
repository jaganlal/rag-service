{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvZGjmr9Rl6w",
        "outputId": "b4f1bae5-87e8-43b6-b648-0473d8738bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3.13\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ynuxRM7_-Nhu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkGT7MDh-mGk"
      },
      "source": [
        "###Call LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-G_ZBmiSJRP",
        "outputId": "d8563a20-61c0-46e4-a84f-43751ef9c115"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2048,\n",
        "    chunk_overlap=128,\n",
        "    length_function=len\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHPIWFXyTsGM",
        "outputId": "f0ced858-670e-449d-c62a-a7103838efb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsupported file type: combined_documents.md\n",
            "Loaded 20 documents from the folder.\n",
            "Split the documents into 46 chunks.\n"
          ]
        }
      ],
      "source": [
        "# 1. Function to load documents from a folder\n",
        "def load_documents(folder_path: str) -> List[Document]:\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if filename.endswith('.pdf'):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif filename.endswith('.docx'):\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {filename}\")\n",
        "            continue\n",
        "        documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "# Load documents from a folder\n",
        "folder_path = \"./dataset\"\n",
        "documents = load_documents(folder_path)\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents from the folder.\")\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Split the documents into {len(splits)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgBuC-Xdu5gL"
      },
      "source": [
        "###Create and persist Chroma vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ov-ElhBUpJB",
        "outputId": "8fcd703f-120c-46db-ba55-2bc1423c4da3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store created and persisted to './chroma_db'\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "embedding_function = OpenAIEmbeddings()\n",
        "collection_name = \"rag_service_collection_nb\"\n",
        "vectorstore = Chroma.from_documents(collection_name=collection_name, documents=splits, embedding=embedding_function, persist_directory=\"./chroma_db\")\n",
        "#db.persist()\n",
        "\n",
        "print(\"Vector store created and persisted to './chroma_db'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"How to understand documents visually?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGkvzzIZvC2R",
        "outputId": "081f1019-f8a3-4dfa-9ed4-22f2d7198cfa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 2, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='and visual document features like a human would.\\n2.2 Integrating Visual features\\nContrastive Vision Language Models. Mapping\\nlatent representations of textual content to corre-\\nsponding representations of visual content has been\\ndone by aligning disjoint visual and text encoders\\nthrough contrastive losses (Radford et al., 2021;\\nZhai et al., 2023). While some OCR capabilities\\nexist in these models, the visual component is often\\nnot optimized for text understanding. The Fine-\\ngrained Interactive Language-Image Pre-training\\n(Yao et al., 2021) framework extends the late inter-\\naction mechanism to cross-modal vision-language\\nmodels, relying on max similarity operations be-\\ntween text tokens and image patches.\\nVisually Rich Document Understanding. To\\ngo beyond text, some document-focused models\\njointly encode text tokens alongside visual or docu-\\nment layout features (Appalaraju et al., 2021; Kim\\net al., 2021; Huang et al., 2022; Tang et al., 2022).\\nLarge Language transformer Models (LLMs) with\\nstrong reasoning capabilities have recently been\\ncombined with Vision Transformers (ViTs) (Doso-\\nvitskiy et al., 2020) to create VLMs (Alayrac et al.,\\n2022; Liu et al., 2023b; Bai et al., 2023; Laurençon\\net al., 2024) where image patch vectors from con-\\ntrastively trained ViT models (Zhai et al., 2023) are\\nfed as input embeddings to the language model and\\nconcatenated with the text-token embeddings.\\nPaliGemma. The PaliGemma-3B model (Beyer\\net al., 2024) extends concepts from Pali3 (Chen\\net al., 2023), and projects SigLIP-So400m/14 (Al-\\nabdulmohsin et al., 2023) patch embeddings into\\nGemma-2B’s text vector space (Gemma Team\\net al., 2024). Along with its reasonable size w.r.t.\\nother performant VLMs, an interesting property of\\nPaliGemma’s text model is that it is fine-tuned with\\nfull-block attention on the prefix (instruction text\\nand image tokens).\\nVLMs display enhanced capabilities in Visual Ques-\\ntion Answering, captioning, and document under-\\nstanding (Yue et al., 2023), but are not optimized\\nfor retrieval tasks.'),\n",
              " Document(metadata={'page': 1, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='document retrieval settings, in which user queries\\nmay require both textual and visual understanding\\nto be correctly matched to relevant documents. We\\nhighlight the shortcomings of current text-centric\\nsystems in these settings.1\\nContribution 2: ColPali. We propose a novel\\nmodel architecture and training strategy based on\\nVision Language Models (VLMs) to efficiently in-\\ndex documents purely from their visual features,\\nallowing for subsequent fast query matching with'),\n",
              " Document(metadata={'page': 1, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='document retrieval settings, in which user queries\\nmay require both textual and visual understanding\\nto be correctly matched to relevant documents. We\\nhighlight the shortcomings of current text-centric\\nsystems in these settings.1\\nContribution 2: ColPali. We propose a novel\\nmodel architecture and training strategy based on\\nVision Language Models (VLMs) to efficiently in-\\ndex documents purely from their visual features,\\nallowing for subsequent fast query matching with'),\n",
              " Document(metadata={'page': 1, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='document retrieval settings, in which user queries\\nmay require both textual and visual understanding\\nto be correctly matched to relevant documents. We\\nhighlight the shortcomings of current text-centric\\nsystems in these settings.1\\nContribution 2: ColPali. We propose a novel\\nmodel architecture and training strategy based on\\nVision Language Models (VLMs) to efficiently in-\\ndex documents purely from their visual features,\\nallowing for subsequent fast query matching with'),\n",
              " Document(metadata={'page': 1, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='document retrieval settings, in which user queries\\nmay require both textual and visual understanding\\nto be correctly matched to relevant documents. We\\nhighlight the shortcomings of current text-centric\\nsystems in these settings.1\\nContribution 2: ColPali. We propose a novel\\nmodel architecture and training strategy based on\\nVision Language Models (VLMs) to efficiently in-\\ndex documents purely from their visual features,\\nallowing for subsequent fast query matching with'),\n",
              " Document(metadata={'page': 2, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='and visual document features like a human would.\\n2.2 Integrating Visual features\\nContrastive Vision Language Models. Mapping\\nlatent representations of textual content to corre-\\nsponding representations of visual content has been\\ndone by aligning disjoint visual and text encoders\\nthrough contrastive losses (Radford et al., 2021;\\nZhai et al., 2023). While some OCR capabilities\\nexist in these models, the visual component is often\\nnot optimized for text understanding. The Fine-'),\n",
              " Document(metadata={'page': 2, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='and visual document features like a human would.\\n2.2 Integrating Visual features\\nContrastive Vision Language Models. Mapping\\nlatent representations of textual content to corre-\\nsponding representations of visual content has been\\ndone by aligning disjoint visual and text encoders\\nthrough contrastive losses (Radford et al., 2021;\\nZhai et al., 2023). While some OCR capabilities\\nexist in these models, the visual component is often\\nnot optimized for text understanding. The Fine-'),\n",
              " Document(metadata={'page': 2, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='and visual document features like a human would.\\n2.2 Integrating Visual features\\nContrastive Vision Language Models. Mapping\\nlatent representations of textual content to corre-\\nsponding representations of visual content has been\\ndone by aligning disjoint visual and text encoders\\nthrough contrastive losses (Radford et al., 2021;\\nZhai et al., 2023). While some OCR capabilities\\nexist in these models, the visual component is often\\nnot optimized for text understanding. The Fine-'),\n",
              " Document(metadata={'page': 0, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='ColPali: Efficient Document Retrieval with Vision Language Models\\nManuel Faysse* 1,3 Hugues Sibille∗1,4 Tony Wu∗1 Bilel Omrani1\\nGautier Viaud1 Céline Hudelot3 Pierre Colombo2,3\\n1Illuin Technology 2Equall.ai\\n3CentraleSupélec, Paris-Saclay 4ETH Zürich\\nmanuel.faysse@centralesupelec.fr\\nAbstract\\nDocuments are visually rich structures that con-\\nvey information through text, as well as tables,\\nfigures, page layouts, or fonts. While mod-\\nern document retrieval systems exhibit strong'),\n",
              " Document(metadata={'page': 0, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='ColPali: Efficient Document Retrieval with Vision Language Models\\nManuel Faysse* 1,3 Hugues Sibille∗1,4 Tony Wu∗1 Bilel Omrani1\\nGautier Viaud1 Céline Hudelot3 Pierre Colombo2,3\\n1Illuin Technology 2Equall.ai\\n3CentraleSupélec, Paris-Saclay 4ETH Zürich\\nmanuel.faysse@centralesupelec.fr\\nAbstract\\nDocuments are visually rich structures that con-\\nvey information through text, as well as tables,\\nfigures, page layouts, or fonts. While mod-\\nern document retrieval systems exhibit strong')]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import FlashrankRerank\n",
        "\n",
        "compressor = FlashrankRerank(model=\"ms-marco-MiniLM-L-12-v2\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 7.54 s, sys: 617 ms, total: 8.16 s\n",
            "Wall time: 1.3 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "reranked_docs = compression_retriever.invoke(query)\n",
        "len(reranked_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id: 1\n",
            "\n",
            "text: document retrieval settings, in which user queries\n",
            "may require both textual and visual understanding\n",
            "to be correctly matched to relevant documents. We\n",
            "highlight the shortcomings of current text-centric\n",
            "systems in these settings.1\n",
            "Contribution 2: ColPali. W\n",
            "\n",
            "score: 0.9555143713951111\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "id: 2\n",
            "\n",
            "text: document retrieval settings, in which user queries\n",
            "may require both textual and visual understanding\n",
            "to be correctly matched to relevant documents. We\n",
            "highlight the shortcomings of current text-centric\n",
            "systems in these settings.1\n",
            "Contribution 2: ColPali. W\n",
            "\n",
            "score: 0.9555143713951111\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "id: 3\n",
            "\n",
            "text: document retrieval settings, in which user queries\n",
            "may require both textual and visual understanding\n",
            "to be correctly matched to relevant documents. We\n",
            "highlight the shortcomings of current text-centric\n",
            "systems in these settings.1\n",
            "Contribution 2: ColPali. W\n",
            "\n",
            "score: 0.9555143713951111\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for doc in reranked_docs:\n",
        "    print(f\"id: {doc.metadata['id']}\\n\")\n",
        "    print(f\"text: {doc.page_content[:256]}\\n\")\n",
        "    print(f\"score: {doc.metadata['relevance_score']}\")\n",
        "    print(\"-\" * 80)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XRiNA4y8vkcz"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template = \"\"\"You are a highly capable assistant specializing in answering questions from visually rich documents. Consider both textual and visual elements as context.\n",
        "\n",
        "Given the context below:\n",
        "{context}\n",
        "\n",
        "And the question:\n",
        "{question}\n",
        "\n",
        "Provide a precise and concise answer based solely on the provided context. Do not include any information that is not explicitly present in the context.\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYCoxqbcvuht",
        "outputId": "aed01866-ba79-4685-cc45-9a060686983d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content=\"You are a highly capable assistant specializing in answering questions from visually rich documents. Consider both textual and visual elements as context.\\n\\nGiven the context below:\\n[Document(metadata={'page': 2, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='and visual document features like a human would.\\\\n2.2 Integrating Visual features\\\\nContrastive Vision Language Models. Mapping\\\\nlatent representations of textual content to corre-\\\\nsponding representations of visual content has been\\\\ndone by aligning disjoint visual and text encoders\\\\nthrough contrastive losses (Radford et al., 2021;\\\\nZhai et al., 2023). While some OCR capabilities\\\\nexist in these models, the visual component is often\\\\nnot optimized for text understanding. The Fine-\\\\ngrained Interactive Language-Image Pre-training\\\\n(Yao et al., 2021) framework extends the late inter-\\\\naction mechanism to cross-modal vision-language\\\\nmodels, relying on max similarity operations be-\\\\ntween text tokens and image patches.\\\\nVisually Rich Document Understanding. To\\\\ngo beyond text, some document-focused models\\\\njointly encode text tokens alongside visual or docu-\\\\nment layout features (Appalaraju et al., 2021; Kim\\\\net al., 2021; Huang et al., 2022; Tang et al., 2022).\\\\nLarge Language transformer Models (LLMs) with\\\\nstrong reasoning capabilities have recently been\\\\ncombined with Vision Transformers (ViTs) (Doso-\\\\nvitskiy et al., 2020) to create VLMs (Alayrac et al.,\\\\n2022; Liu et al., 2023b; Bai et al., 2023; Laurençon\\\\net al., 2024) where image patch vectors from con-\\\\ntrastively trained ViT models (Zhai et al., 2023) are\\\\nfed as input embeddings to the language model and\\\\nconcatenated with the text-token embeddings.\\\\nPaliGemma. The PaliGemma-3B model (Beyer\\\\net al., 2024) extends concepts from Pali3 (Chen\\\\net al., 2023), and projects SigLIP-So400m/14 (Al-\\\\nabdulmohsin et al., 2023) patch embeddings into\\\\nGemma-2B’s text vector space (Gemma Team\\\\net al., 2024). Along with its reasonable size w.r.t.\\\\nother performant VLMs, an interesting property of\\\\nPaliGemma’s text model is that it is fine-tuned with\\\\nfull-block attention on the prefix (instruction text\\\\nand image tokens).\\\\nVLMs display enhanced capabilities in Visual Ques-\\\\ntion Answering, captioning, and document under-\\\\nstanding (Yue et al., 2023), but are not optimized\\\\nfor retrieval tasks.'), Document(metadata={'page': 1, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='document retrieval settings, in which user queries\\\\nmay require both textual and visual understanding\\\\nto be correctly matched to relevant documents. We\\\\nhighlight the shortcomings of current text-centric\\\\nsystems in these settings.1\\\\nContribution 2: ColPali. We propose a novel\\\\nmodel architecture and training strategy based on\\\\nVision Language Models (VLMs) to efficiently in-\\\\ndex documents purely from their visual features,\\\\nallowing for subsequent fast query matching with'), Document(metadata={'page': 1, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='document retrieval settings, in which user queries\\\\nmay require both textual and visual understanding\\\\nto be correctly matched to relevant documents. We\\\\nhighlight the shortcomings of current text-centric\\\\nsystems in these settings.1\\\\nContribution 2: ColPali. We propose a novel\\\\nmodel architecture and training strategy based on\\\\nVision Language Models (VLMs) to efficiently in-\\\\ndex documents purely from their visual features,\\\\nallowing for subsequent fast query matching with'), Document(metadata={'page': 1, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='document retrieval settings, in which user queries\\\\nmay require both textual and visual understanding\\\\nto be correctly matched to relevant documents. We\\\\nhighlight the shortcomings of current text-centric\\\\nsystems in these settings.1\\\\nContribution 2: ColPali. We propose a novel\\\\nmodel architecture and training strategy based on\\\\nVision Language Models (VLMs) to efficiently in-\\\\ndex documents purely from their visual features,\\\\nallowing for subsequent fast query matching with'), Document(metadata={'page': 1, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='document retrieval settings, in which user queries\\\\nmay require both textual and visual understanding\\\\nto be correctly matched to relevant documents. We\\\\nhighlight the shortcomings of current text-centric\\\\nsystems in these settings.1\\\\nContribution 2: ColPali. We propose a novel\\\\nmodel architecture and training strategy based on\\\\nVision Language Models (VLMs) to efficiently in-\\\\ndex documents purely from their visual features,\\\\nallowing for subsequent fast query matching with'), Document(metadata={'page': 2, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='and visual document features like a human would.\\\\n2.2 Integrating Visual features\\\\nContrastive Vision Language Models. Mapping\\\\nlatent representations of textual content to corre-\\\\nsponding representations of visual content has been\\\\ndone by aligning disjoint visual and text encoders\\\\nthrough contrastive losses (Radford et al., 2021;\\\\nZhai et al., 2023). While some OCR capabilities\\\\nexist in these models, the visual component is often\\\\nnot optimized for text understanding. The Fine-'), Document(metadata={'page': 2, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='and visual document features like a human would.\\\\n2.2 Integrating Visual features\\\\nContrastive Vision Language Models. Mapping\\\\nlatent representations of textual content to corre-\\\\nsponding representations of visual content has been\\\\ndone by aligning disjoint visual and text encoders\\\\nthrough contrastive losses (Radford et al., 2021;\\\\nZhai et al., 2023). While some OCR capabilities\\\\nexist in these models, the visual component is often\\\\nnot optimized for text understanding. The Fine-'), Document(metadata={'page': 2, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='and visual document features like a human would.\\\\n2.2 Integrating Visual features\\\\nContrastive Vision Language Models. Mapping\\\\nlatent representations of textual content to corre-\\\\nsponding representations of visual content has been\\\\ndone by aligning disjoint visual and text encoders\\\\nthrough contrastive losses (Radford et al., 2021;\\\\nZhai et al., 2023). While some OCR capabilities\\\\nexist in these models, the visual component is often\\\\nnot optimized for text understanding. The Fine-'), Document(metadata={'page': 0, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='ColPali: Efficient Document Retrieval with Vision Language Models\\\\nManuel Faysse* 1,3 Hugues Sibille∗1,4 Tony Wu∗1 Bilel Omrani1\\\\nGautier Viaud1 Céline Hudelot3 Pierre Colombo2,3\\\\n1Illuin Technology 2Equall.ai\\\\n3CentraleSupélec, Paris-Saclay 4ETH Zürich\\\\nmanuel.faysse@centralesupelec.fr\\\\nAbstract\\\\nDocuments are visually rich structures that con-\\\\nvey information through text, as well as tables,\\\\nfigures, page layouts, or fonts. While mod-\\\\nern document retrieval systems exhibit strong'), Document(metadata={'page': 0, 'source': './dataset/ColPali_2407.01449v3.pdf'}, page_content='ColPali: Efficient Document Retrieval with Vision Language Models\\\\nManuel Faysse* 1,3 Hugues Sibille∗1,4 Tony Wu∗1 Bilel Omrani1\\\\nGautier Viaud1 Céline Hudelot3 Pierre Colombo2,3\\\\n1Illuin Technology 2Equall.ai\\\\n3CentraleSupélec, Paris-Saclay 4ETH Zürich\\\\nmanuel.faysse@centralesupelec.fr\\\\nAbstract\\\\nDocuments are visually rich structures that con-\\\\nvey information through text, as well as tables,\\\\nfigures, page layouts, or fonts. While mod-\\\\nern document retrieval systems exhibit strong')]\\n\\nAnd the question:\\nHow to understand documents visually?\\n\\nProvide a precise and concise answer based solely on the provided context. Do not include any information that is not explicitly present in the context.\\n\\nAnswer:\", additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from langchain.schema.runnable import RunnablePassthrough\n",
        "# rag_chain = (\n",
        "#     {\"context\": compression_retriever, \"question\": RunnablePassthrough()} | prompt\n",
        "# )\n",
        "# rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uztRXXwDvud9"
      },
      "outputs": [],
      "source": [
        "def docs2str(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk-VtijcwZw9",
        "outputId": "b976c90d-5d8e-427b-cee7-2281167b746c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='You are a highly capable assistant specializing in answering questions from visually rich documents. Consider both textual and visual elements as context.\\n\\nGiven the context below:\\ndocument retrieval settings, in which user queries\\nmay require both textual and visual understanding\\nto be correctly matched to relevant documents. We\\nhighlight the shortcomings of current text-centric\\nsystems in these settings.1\\nContribution 2: ColPali. We propose a novel\\nmodel architecture and training strategy based on\\nVision Language Models (VLMs) to efficiently in-\\ndex documents purely from their visual features,\\nallowing for subsequent fast query matching with\\n\\ndocument retrieval settings, in which user queries\\nmay require both textual and visual understanding\\nto be correctly matched to relevant documents. We\\nhighlight the shortcomings of current text-centric\\nsystems in these settings.1\\nContribution 2: ColPali. We propose a novel\\nmodel architecture and training strategy based on\\nVision Language Models (VLMs) to efficiently in-\\ndex documents purely from their visual features,\\nallowing for subsequent fast query matching with\\n\\ndocument retrieval settings, in which user queries\\nmay require both textual and visual understanding\\nto be correctly matched to relevant documents. We\\nhighlight the shortcomings of current text-centric\\nsystems in these settings.1\\nContribution 2: ColPali. We propose a novel\\nmodel architecture and training strategy based on\\nVision Language Models (VLMs) to efficiently in-\\ndex documents purely from their visual features,\\nallowing for subsequent fast query matching with\\n\\nAnd the question:\\nHow to understand documents visually?\\n\\nProvide a precise and concise answer based solely on the provided context. Do not include any information that is not explicitly present in the context.\\n\\nAnswer:', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from langchain.schema.runnable import RunnablePassthrough\n",
        "# rag_chain = (\n",
        "#     {\"context\": compression_retriever | docs2str, \"question\": RunnablePassthrough()} | prompt\n",
        "# )\n",
        "# rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gjDAr3lwbXA",
        "outputId": "7a9df563-e772-401a-be4f-7abecb58ce40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To understand documents visually, one can utilize a model architecture and training strategy based on Vision Language Models (VLMs) that efficiently index documents purely from their visual features. This approach allows for fast query matching by leveraging both textual and visual understanding to correctly match user queries to relevant documents.\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "rag_chain = (\n",
        "    {\"context\": compression_retriever | docs2str, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "question = query\n",
        "response = rag_chain.invoke(question)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RPDMn9q4GbE"
      },
      "source": [
        "###Conversational RAG\n",
        "\n",
        "####Handling Follow Up Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0OW5ak-Bqf1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbbjrk1cwl19"
      },
      "outputs": [],
      "source": [
        "# Example conversation\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "chat_history = []\n",
        "chat_history.extend([\n",
        "    HumanMessage(content=question),\n",
        "    AIMessage(content=response)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApN5nctt2XSn",
        "outputId": "61d19ef0-2efb-44df-f098-1754cb19be2d"
      },
      "outputs": [],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Sm2ju8n72YXY",
        "outputId": "eebba764-fee0-412a-dafc-528d5cb49d6f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# history_aware_retriever = create_history_aware_retriever(\n",
        "#     llm, retriever, contextualize_q_prompt\n",
        "# )\n",
        "contextualize_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
        "contextualize_chain.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\": chat_history})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vShrBX-M24_r",
        "outputId": "77755e1c-890c-49e8-deb0-daa9de3ae6fe"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "history_aware_retriever.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\": chat_history})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0BMwLBo7jhV",
        "outputId": "801db6d2-0cf6-448d-81ae-bfd90ce9d479"
      },
      "outputs": [],
      "source": [
        "retriever.invoke(\"Where it is headquartered?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AYJ1jce6cbQ"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.\"),\n",
        "    #  (\"system\", \"Tell me joke on Programming\"),\n",
        "    (\"system\", \"Context: {context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkeEE0hD65zW",
        "outputId": "accc91c8-5db0-418f-fdec-114ae6f7f47c"
      },
      "outputs": [],
      "source": [
        "rag_chain.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\":chat_history})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qdzx68u5aCv"
      },
      "source": [
        "###Building Multi User Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWufJa-c5HaC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftIORKEF3coG"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "DB_NAME = \"rag_app.db\"\n",
        "\n",
        "def get_db_connection():\n",
        "    conn = sqlite3.connect(DB_NAME)\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    return conn\n",
        "\n",
        "def create_application_logs():\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
        "                    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                     session_id TEXT,\n",
        "                     user_query TEXT,\n",
        "                     gpt_response TEXT,\n",
        "                     model TEXT,\n",
        "                     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
        "    conn.close()\n",
        "\n",
        "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
        "                 (session_id, user_query, gpt_response, model))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def get_chat_history(session_id):\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
        "    messages = []\n",
        "    for row in cursor.fetchall():\n",
        "        messages.extend([\n",
        "            {\"role\": \"human\", \"content\": row['user_query']},\n",
        "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
        "        ])\n",
        "    conn.close()\n",
        "    return messages\n",
        "\n",
        "# Initialize the database\n",
        "create_application_logs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M--sjKYN6JIQ",
        "outputId": "1461b818-15d8-4f67-c6ac-4a10b0f7a5d3"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "session_id = str(uuid.uuid4())\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "question1 = \"When was GreenGrow Innovations founded?\"\n",
        "answer1 = rag_chain.invoke({\"input\": question1, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question1, answer1, \"gpt-4-o-mini\")\n",
        "print(f\"Human: {question1}\")\n",
        "print(f\"AI: {answer1}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqDRm5Rl6P5U",
        "outputId": "39944daf-182e-4148-85dd-83ac766e637e"
      },
      "outputs": [],
      "source": [
        "question2 = \"Where it is headquartered?\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question2, answer2, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question2}\")\n",
        "print(f\"AI: {answer2}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgQiUzN68KYl"
      },
      "source": [
        "New User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jWmJ0FgD78AO"
      },
      "outputs": [],
      "source": [
        "session_id = str(uuid.uuid4())\n",
        "question = \"What is GreenGrow\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer = rag_chain.invoke({\"input\": question, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question, answer, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question}\")\n",
        "print(f\"AI: {answer}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ssvjqblJ8SC3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
