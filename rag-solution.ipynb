{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import os\n",
    "from llama_parse import LlamaParse\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from flashrank import Ranker\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wn/pgggjbrx3czcwhshwnyfbk2m0000gn/T/ipykernel_88413/2511525932.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n",
      "/var/folders/wn/pgggjbrx3czcwhshwnyfbk2m0000gn/T/ipykernel_88413/2511525932.py:3: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
      "INFO:flashrank.Ranker:Downloading ms-marco-TinyBERT-L-2-v2...\n",
      "ms-marco-TinyBERT-L-2-v2.zip: 100%|██████████| 3.26M/3.26M [00:01<00:00, 2.91MiB/s]\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "ranker = Ranker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVE_DIR = \"./archives\"\n",
    "def load_or_initialize_indices():\n",
    "  try:\n",
    "    with open(ARCHIVE_DIR+\"/bm25_index.pkl\", \"rb\") as f:\n",
    "      bm25_index = pickle.load(f)\n",
    "  except FileNotFoundError:\n",
    "    bm25_index = BM25Okapi([])\n",
    "\n",
    "  try:\n",
    "    faiss_index = FAISS.load_local(ARCHIVE_DIR+\"/faiss_index\", embeddings)\n",
    "  except FileNotFoundError:\n",
    "    faiss_index = None\n",
    "\n",
    "  return bm25_index, faiss_index\n",
    "\n",
    "def save_indices(bm25_index, faiss_index):\n",
    "  with open(ARCHIVE_DIR+\"/bm25_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bm25_index, f)\n",
    "  faiss_index.save_local(ARCHIVE_DIR+\"/faiss_index\")\n",
    "\n",
    "def rerank_documents(query, documents):\n",
    "  reranked_docs = ranker.rerank(query, documents)\n",
    "  return reranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"The provided document is a PDF file containing structured and unstructured content.\n",
    "It may include financial information, tables, management discussions, and analyses.\n",
    "Try to capture the essence of the document, including text, tables, and key highlights.\n",
    "Be precise and ensure data integrity while processing.\"\"\"\n",
    "\n",
    "async def parse_pdf(file_path: str):\n",
    "  parser = LlamaParse(\n",
    "      result_type=\"markdown\",\n",
    "      parsing_instruction=instruction,\n",
    "      max_timeout=5000,\n",
    "  )\n",
    "  return await parser.aload_data(file_path)\n",
    "\n",
    "async def ingest_documents(files: List[str]):\n",
    "  try:\n",
    "    parsed_docs = []\n",
    "    for file_path in files:\n",
    "      root, ext = os.path.splitext(file_path)\n",
    "      if ext == \".pdf\":\n",
    "        with open(file_path, \"rb\") as file:\n",
    "          parsed_content = await parse_pdf(file_path)\n",
    "          parsed_docs.extend(parsed_content)\n",
    "\n",
    "    documents = []\n",
    "    for doc in parsed_docs:\n",
    "      md_content = doc.to_markdown()\n",
    "      chunks = text_splitter.split_text(md_content)\n",
    "      documents.extend(chunks)\n",
    "\n",
    "    bm25_index, faiss_index = load_or_initialize_indices()\n",
    "\n",
    "    new_corpus = bm25_index.corpus + [doc.split() for doc in documents]\n",
    "    bm25_index = BM25Okapi(new_corpus)\n",
    "\n",
    "    if faiss_index:\n",
    "      faiss_index.add_texts(documents)\n",
    "    else:\n",
    "      faiss_index = FAISS.from_texts(documents, embeddings)\n",
    "\n",
    "    save_indices(bm25_index, faiss_index)\n",
    "\n",
    "    return {\"message\": \"Documents ingested and indexed successfully.\"}\n",
    "  except Exception as e:\n",
    "    raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query for answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query(BaseModel):\n",
    "  question: str\n",
    "\n",
    "def query_documents(query: Query):\n",
    "  try:\n",
    "    bm25_index, faiss_index = load_or_initialize_indices()\n",
    "\n",
    "    bm25_docs = bm25_index.get_top_n(query.question.split(), bm25_index.corpus, n=5)\n",
    "\n",
    "    faiss_docs = faiss_index.similarity_search(query.question, k=5)\n",
    "\n",
    "    combined_docs = list(set(bm25_docs + faiss_docs))\n",
    "    reranked_docs = rerank_documents(query.question, combined_docs)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=faiss_index.as_retriever())\n",
    "    answer = qa_chain.run(query.question)\n",
    "\n",
    "    return {\"answer\": answer, \"reranked_docs\": reranked_docs}\n",
    "  except Exception as e:\n",
    "    raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cloud.llamaindex.ai/api/parsing/upload \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 0320b55d-cd25-4023-92f7-569feb2f5819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.cloud.llamaindex.ai/api/parsing/job/0320b55d-cd25-4023-92f7-569feb2f5819 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.cloud.llamaindex.ai/api/parsing/job/0320b55d-cd25-4023-92f7-569feb2f5819 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.cloud.llamaindex.ai/api/parsing/job/0320b55d-cd25-4023-92f7-569feb2f5819 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.cloud.llamaindex.ai/api/parsing/job/0320b55d-cd25-4023-92f7-569feb2f5819/result/markdown \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "HTTPException",
     "evalue": "500: 'Document' object has no attribute 'to_markdown'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 26\u001b[0m, in \u001b[0;36mingest_documents\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m parsed_docs:\n\u001b[0;32m---> 26\u001b[0m   md_content \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_markdown\u001b[49m()\n\u001b[1;32m     27\u001b[0m   chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(md_content)\n",
      "File \u001b[0;32m~/workspace/rag-service/env/lib/python3.11/site-packages/pydantic/main.py:856\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Document' object has no attribute 'to_markdown'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHTTPException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./dataset/ColPali_2407.01449v3.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m ingest_documents(files)\n",
      "Cell \u001b[0;32mIn[18], line 44\u001b[0m, in \u001b[0;36mingest_documents\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocuments ingested and indexed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 44\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m HTTPException(status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[0;31mHTTPException\u001b[0m: 500: 'Document' object has no attribute 'to_markdown'"
     ]
    }
   ],
   "source": [
    "files = [\"./dataset/ColPali_2407.01449v3.pdf\"]\n",
    "await ingest_documents(files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
