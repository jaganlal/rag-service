{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### https://medium.com/the-ai-forum/implementing-contextual-retrieval-in-rag-pipeline-8f1bc7cbd5e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import getpass\n",
    "from typing import List, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from langchain.retrievers import ContextualCompressionRetriever,BM25Retriever,EnsembleRetriever\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers.embeddings_redundant_filter import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import time\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from whoosh.index import create_in, open_dir, exists_in\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.qparser import QueryParser\n",
    "from pydantic import Field\n",
    "from typing import List\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhooshIndexManager:\n",
    "    def __init__(self, index_dir: str = \"./storage/bm25_index\"):\n",
    "        \"\"\"\n",
    "        Initialize the Whoosh index manager.\n",
    "\n",
    "        Args:\n",
    "            index_dir (str): Directory to store the Whoosh index.\n",
    "        \"\"\"\n",
    "        self.index_dir = index_dir\n",
    "        self.schema = Schema(\n",
    "            doc_id=ID(stored=True, unique=True),\n",
    "            content=TEXT(stored=True)\n",
    "        )\n",
    "        self.index = self._initialize_index()\n",
    "\n",
    "    def _initialize_index(self):\n",
    "        \"\"\"Initialize or load the index.\"\"\"\n",
    "        if not os.path.exists(self.index_dir):\n",
    "            # Create the directory if it doesn't exist\n",
    "            os.makedirs(self.index_dir, exist_ok=True)\n",
    "            print(f\"Created directory: {self.index_dir}\")\n",
    "            # Create a new index\n",
    "            return create_in(self.index_dir, self.schema)\n",
    "        elif exists_in(self.index_dir):\n",
    "            # Open the existing index\n",
    "            print(f\"Loading existing index from: {self.index_dir}\")\n",
    "            return open_dir(self.index_dir)\n",
    "        else:\n",
    "            # Directory exists, but it's not a valid Whoosh index\n",
    "            raise ValueError(\n",
    "                f\"The directory '{self.index_dir}' exists but does not contain a valid Whoosh index. \"\n",
    "                \"Delete the directory or ensure it contains a valid index.\"\n",
    "            )\n",
    "\n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"Add documents to the index.\"\"\"\n",
    "        writer = self.index.writer()\n",
    "        for i, doc in enumerate(documents):\n",
    "            writer.add_document(doc_id=str(i), content=doc.page_content)\n",
    "        writer.commit()\n",
    "        print(f\"Added {len(documents)} documents to the index.\")\n",
    "\n",
    "    def search(self, query: str, top_n: int = 10) -> List[dict]:\n",
    "        \"\"\"Search the index.\"\"\"\n",
    "        with self.index.searcher() as searcher:\n",
    "            parser = QueryParser(\"content\", schema=self.schema)\n",
    "            parsed_query = parser.parse(query)\n",
    "            results = searcher.search(parsed_query, limit=top_n)\n",
    "            return [\n",
    "                {\"content\": result[\"content\"], \"doc_id\": result[\"doc_id\"]}\n",
    "                for result in results\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhooshRetriever(BaseRetriever):\n",
    "    index_manager: object = Field(...)\n",
    "    k: int = Field(default=10)\n",
    "\n",
    "    def __init__(self, index_manager, k: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize the Whoosh retriever.\n",
    "\n",
    "        Args:\n",
    "            index_manager (WhooshIndexManager): The Whoosh index manager.\n",
    "            k (int): Number of documents to retrieve.\n",
    "        \"\"\"\n",
    "        super().__init__(index_manager=index_manager,\n",
    "                         k=k)  # Initialize the BaseRetriever with Pydantic fields\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"Retrieve documents in LangChain-compatible format.\"\"\"\n",
    "        results = self.index_manager.search(query, top_n=self.k)\n",
    "        return [\n",
    "            Document(page_content=result[\"content\"], metadata={\n",
    "                     \"doc_id\": result[\"doc_id\"]})\n",
    "            for result in results\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example documents\n",
    "# documents = [\n",
    "#     {\"content\": \"This is the first document.\"},\n",
    "#     {\"content\": \"This is the second document.\"},\n",
    "#     {\"content\": \"This is the third document.\"},\n",
    "# ]\n",
    "\n",
    "# # Initialize the Whoosh index manager\n",
    "# index_manager = WhooshIndexManager(index_dir=\"./storage/bm25_index\")\n",
    "\n",
    "# # Add documents to the index\n",
    "# index_manager.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the Whoosh retriever\n",
    "# whoosh_retriever = WhooshRetriever(index_manager=index_manager, k=10)\n",
    "\n",
    "# # Retrieve documents based on a query\n",
    "# query = \"second document\"\n",
    "# results = whoosh_retriever._get_relevant_documents(query)\n",
    "# for result in results:\n",
    "#     print(result.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage Class - FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFAISSStorage:\n",
    "    def __init__(self, storage_path: str, embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            storage_path (str): Path to the directory where the FAISS index will be saved/loaded.\n",
    "            embeddings: Embeddings model (e.g., OpenAIEmbeddings, SentenceTransformerEmbeddings).\n",
    "        \"\"\"\n",
    "        self.storage_path = storage_path\n",
    "        self.embeddings = embeddings\n",
    "        self.vectorstore = None\n",
    "\n",
    "    def _index_exists(self) -> bool:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            bool: True if the index exists, False otherwise.\n",
    "        \"\"\"\n",
    "        return os.path.exists(os.path.join(self.storage_path, \"index.faiss\"))\n",
    "\n",
    "    def _load_or_create_index(self):\n",
    "        if self._index_exists():\n",
    "            print(\"Loading existing FAISS index...\")\n",
    "            self.vectorstore = FAISS.load_local(self.storage_path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            print(\"Creating new FAISS index...\")\n",
    "            self.vectorstore = FAISS.from_texts([\"\"], self.embeddings, metadatas=[{}])\n",
    "            self.save_index()\n",
    "\n",
    "    def load_index(self):\n",
    "        \"\"\"\n",
    "        Raises:\n",
    "            ValueError: If the index does not exist.\n",
    "        \"\"\"\n",
    "        if not self._index_exists():\n",
    "            raise ValueError(\"FAISS index does not exist at the specified storage path.\")\n",
    "\n",
    "        print(\"Loading existing FAISS index...\")\n",
    "        self.vectorstore = FAISS.load_local(self.storage_path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    def get_index(self):\n",
    "      self.load_index()\n",
    "      return self.vectorstore\n",
    "\n",
    "    def add_documents(self, documents: list[Document]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            documents (list[Document]): List of Document objects to add to the index.\n",
    "        \"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            self._load_or_create_index()\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to the FAISS index...\")\n",
    "        self.vectorstore.add_documents(documents)\n",
    "        return self.vectorstore\n",
    "\n",
    "    def save_index(self):\n",
    "        if self.vectorstore is None:\n",
    "            raise ValueError(\n",
    "                \"FAISS index not initialized. Add documents first.\")\n",
    "\n",
    "        print(\"Saving FAISS index...\")\n",
    "        self.vectorstore.save_local(self.storage_path)\n",
    "\n",
    "    def query(self, query: str, k: int = 5) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query (str): The query string.\n",
    "            k (int): Number of documents to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            list[Document]: List of Document objects most similar to the query.\n",
    "        \"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            raise ValueError(\n",
    "                \"FAISS index not initialized. Add documents first.\")\n",
    "\n",
    "        print(f\"Querying FAISS index for: '{query}'\")\n",
    "        return self.vectorstore.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRAGPipeline:\n",
    "    def __init__(self, create_contextual_rag = False, vectorstore_path = \"./storage/faiss_local\"):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=800,\n",
    "            chunk_overlap=100,\n",
    "        )\n",
    "        #self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "        self.vectorstore_path = vectorstore_path\n",
    "        self.create_contextual_rag = create_contextual_rag\n",
    "        model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "        model_kwargs = {'device': 'cpu'}\n",
    "        encode_kwargs = {'normalize_embeddings': False}\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs\n",
    "        )\n",
    "        # self.llm = ChatOpenAI(\n",
    "        #     model=\"gpt-4o\",\n",
    "        #     temperature=0,\n",
    "        #     max_tokens=None,\n",
    "        #     timeout=None,\n",
    "        #     max_retries=2,\n",
    "        # )\n",
    "\n",
    "        # self.llm = ChatGroq(\n",
    "        #     model=\"llama-3.2-3b-preview\",\n",
    "        #     temperature=0,\n",
    "        #     max_tokens=None,\n",
    "        #     timeout=None,\n",
    "        #     max_retries=2,\n",
    "        # )\n",
    "\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"mixtral-8x7b-32768\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "        )\n",
    "        self.storage_class: MyFAISSStorage = MyFAISSStorage(self.vectorstore_path, self.embeddings)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-3B-Instruct\")\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "    \n",
    "    def process_document(self, document: str) -> Tuple[List[Document], List[Document]]:\n",
    "        chunks = self.text_splitter.create_documents([document])\n",
    "        contextualized_chunks = self._generate_contextualized_chunks(document, chunks)\n",
    "        return chunks, contextualized_chunks\n",
    "\n",
    "    def split_document(self, document: str, max_retries: int = 1, delay: int = 60) -> Tuple[List[Document], List[Document]]:\n",
    "        chunks = self.text_splitter.split_documents(document)\n",
    "        print(f\"Total number of chunks in document: {len(chunks)}\")\n",
    "        contextualized_chunks = []\n",
    "        if self.create_contextual_rag:\n",
    "          contextualized_chunks = self._generate_contextualized_chunks(document, chunks, max_retries, delay)\n",
    "        return chunks, contextualized_chunks\n",
    "\n",
    "    def _generate_contextualized_chunks(self, document: str, chunks: List[Document], max_retries: int = 1, delay: int = 60) -> List[Document]:\n",
    "        contextualized_chunks = []\n",
    "        for chunk in chunks:\n",
    "            retries = 0\n",
    "            while retries <= max_retries:\n",
    "                try:\n",
    "                    context = self._generate_context(document, chunk.page_content)\n",
    "                    contextualized_content = f\"{context}\\n\\n{chunk.page_content}\"\n",
    "                    contextualized_chunks.append(Document(page_content=contextualized_content, metadata=chunk.metadata))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if \"rate limit\" in str(e).lower() or \"exceeded\" in str(e).lower() or \"quota\" in str(e).lower():\n",
    "                        retries += 1\n",
    "                        if retries > max_retries:\n",
    "                            print(f\"Max retries ({max_retries}) exceeded for chunk: {chunk.page_content[:50]}...\")\n",
    "                            raise e\n",
    "                        delay_with_randomness = delay + random.random()\n",
    "                        print(f\"Rate limit error: {e}. Retrying chunk in {delay_with_randomness:.2f} seconds...\")\n",
    "                        time.sleep(delay_with_randomness)\n",
    "                    else:\n",
    "                        print(f\"Error processing chunk: {chunk.page_content[:50]}... Error: {e}\")\n",
    "                        raise e\n",
    "        return contextualized_chunks\n",
    "\n",
    "    def _generate_context(self, document: str, chunk: str) -> str:\n",
    "        relevant_document = self._extract_relevant_part(document, chunk)\n",
    "\n",
    "        print(f\"Length of the relevant document: {len(relevant_document)}\")\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant specializing in document analysis. Your task is to provide brief, relevant context for a chunk of text from the whitepaper report.\n",
    "        Here is the whitepaper:\n",
    "        <document>\n",
    "        {document}\n",
    "        </document>\n",
    "\n",
    "        Here is the chunk we want to situate within the whole document::\n",
    "        <chunk>\n",
    "        {chunk}\n",
    "        </chunk>\n",
    "\n",
    "        Provide a concise context (2-3 sentences) for this chunk, considering the following guidelines:\n",
    "        Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\n",
    "\n",
    "        Context:\n",
    "        \"\"\")\n",
    "        messages = prompt.format_messages(document=relevant_document, chunk=chunk)\n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "    def _extract_relevant_part(self, document: List[Document], chunk: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract a relevant part of the document for context generation.\n",
    "        This reduces the number of tokens sent to the LLM.\n",
    "        \"\"\"\n",
    "        # Find the position of the chunk in the document\n",
    "        chunk_start = document[0].page_content.find(chunk)\n",
    "        chunk_end = chunk_start + len(chunk)\n",
    "        \n",
    "        # Extract a window of text around the chunk (e.g., 2048 characters before and after)\n",
    "        window_size = 2048\n",
    "        start = max(0, chunk_start - window_size)\n",
    "        end = min(len(document[0].page_content), chunk_end + window_size)\n",
    "        \n",
    "        return document[0].page_content[start:end]\n",
    "\n",
    "    def _extract_relevant_part_with_maxtokens(self, document: List[Document], chunk: str, max_tokens: int = 2048) -> str:\n",
    "        chunk_tokens = len(self.tokenizer.encode(chunk))\n",
    "        \n",
    "        # Calculate the maximum allowed tokens for context\n",
    "        max_context_tokens = max_tokens - chunk_tokens\n",
    "        \n",
    "        # Estimate the number of characters per token (average is ~4 characters per token)\n",
    "        chars_per_token = 4\n",
    "        max_context_chars = max_context_tokens * chars_per_token\n",
    "        \n",
    "        # Find the position of the chunk in the document\n",
    "        chunk_start = document[0].page_content.find(chunk)\n",
    "        chunk_end = chunk_start + len(chunk)\n",
    "        \n",
    "        # Extract a window of text around the chunk\n",
    "        window_size = min(max_context_chars, len(document[0].page_content))\n",
    "        start = max(0, chunk_start - window_size // 2)\n",
    "        end = min(len(document[0].page_content), chunk_end + window_size // 2)\n",
    "        \n",
    "        return document[0].page_content[start:end]\n",
    "\n",
    "    def create_inmemory_vectorstores(self, chunks: List[Document]) -> FAISS:\n",
    "        return FAISS.from_documents(chunks, self.embeddings)\n",
    "\n",
    "    def create_vectorstores(self, chunks: List[Document]) -> FAISS:\n",
    "        return self.storage_class.add_documents(chunks)\n",
    "\n",
    "    def get_vectorstores(self):\n",
    "        return self.storage_class.get_index()\n",
    "\n",
    "    def save_vectorstores(self):\n",
    "        self.storage_class.save_index()\n",
    "\n",
    "    def create_bm25_index(self, chunks: List[Document]) -> BM25Okapi:\n",
    "        tokenized_chunks = [chunk.page_content.split() for chunk in chunks]\n",
    "        return BM25Okapi(tokenized_chunks)\n",
    "    \n",
    "    def create_flashrank_index(self,vectorstore):\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\":20})\n",
    "        compression_retriever = ContextualCompressionRetriever(base_compressor=FlashrankRerank(), base_retriever=retriever)\n",
    "        return compression_retriever\n",
    "\n",
    "    def create_bm25_retriever(self, chunks: List[Document]) -> BM25Retriever:\n",
    "        bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "        return bm25_retriever\n",
    "    \n",
    "    def create_ensemble_retriever_reranker(self, vectorstore, bm25_retriever) -> EnsembleRetriever:\n",
    "        retriever_vs = vectorstore.as_retriever(search_kwargs={\"k\":20})\n",
    "        bm25_retriever.k =10\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[retriever_vs, bm25_retriever],\n",
    "            weights=[0.5, 0.5]\n",
    "        )\n",
    "        redundant_filter = EmbeddingsRedundantFilter(embeddings=self.embeddings)\n",
    "        reranker = FlashrankRerank()\n",
    "        pipeline_compressor = DocumentCompressorPipeline(transformers=[redundant_filter, reranker])\n",
    "        compression_pipeline = ContextualCompressionRetriever(base_compressor=pipeline_compressor,\n",
    "                                                      base_retriever=ensemble_retriever)\n",
    "        return compression_pipeline\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_cache_key(document: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a cache key for a document.\n",
    "        \"\"\"\n",
    "        return hashlib.md5(document.encode()).hexdigest()\n",
    "\n",
    "    def generate_answer(self, query: str, relevant_chunks: List[str]) -> str:\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Based on the following information, please provide a concise and accurate answer to the question.\n",
    "        If the information is not sufficient to answer the question, say so.\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Relevant information:\n",
    "        {chunks}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\")\n",
    "        messages = prompt.format_messages(query=query, chunks=\"\\n\\n\".join(relevant_chunks))\n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleRetrieverReranker:\n",
    "    def __init__(self, embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: Embeddings model (e.g., OpenAIEmbeddings, SentenceTransformerEmbeddings).\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def _convert_numpy_to_float(self, obj):\n",
    "        \"\"\"\n",
    "        Recursively converts numpy.float32 values in a dictionary, list, or other object to native Python float.\n",
    "\n",
    "        Args:\n",
    "            obj: A dictionary, list, or other object.\n",
    "\n",
    "        Returns:\n",
    "            The same object with numpy.float32 values converted to float.\n",
    "        \"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: self._convert_numpy_to_float(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_numpy_to_float(item) for item in obj]\n",
    "        elif isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            # Convert numpy arrays to lists of floats\n",
    "            return obj.astype(float).tolist()\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def _process_document(self, doc):\n",
    "        \"\"\"\n",
    "        Processes a Document object to ensure all numpy.float32 values are converted to float.\n",
    "\n",
    "        Args:\n",
    "            doc: A Document object.\n",
    "\n",
    "        Returns:\n",
    "            Document: A processed Document object.\n",
    "        \"\"\"\n",
    "        # Convert metadata\n",
    "        if hasattr(doc, \"metadata\") and doc.metadata:\n",
    "            doc.metadata = self._convert_numpy_to_float(doc.metadata)\n",
    "\n",
    "        # Convert embeddings (if present)\n",
    "        if hasattr(doc, \"embedding\") and isinstance(doc.embedding, (np.ndarray, np.float32)):\n",
    "            doc.embedding = self._convert_numpy_to_float(doc.embedding)\n",
    "\n",
    "        return doc\n",
    "\n",
    "    def create_ensemble_retriever_reranker(self, vectorstore, bm25_retriever):\n",
    "        \"\"\"\n",
    "        Creates an ensemble retriever with a reranker.\n",
    "\n",
    "        Args:\n",
    "            vectorstore: Initialized FAISS vectorstore.\n",
    "            bm25_retriever: BM25 retriever to combine with the FAISS retriever.\n",
    "\n",
    "        Returns:\n",
    "            ContextualCompressionRetriever: An ensemble retriever with a reranker.\n",
    "        \"\"\"\n",
    "        if vectorstore is None:\n",
    "            raise ValueError(\n",
    "                \"FAISS vectorstore is not initialized. Ensure the vectorstore is loaded or created.\")\n",
    "\n",
    "        # Ensure the vectorstore is initialized\n",
    "        retriever_vs = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "        bm25_retriever.k = 10\n",
    "\n",
    "        # Create ensemble retriever\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[retriever_vs, bm25_retriever],\n",
    "            weights=[0.5, 0.5]\n",
    "        )\n",
    "\n",
    "        # Add reranker and compression pipeline\n",
    "        redundant_filter = EmbeddingsRedundantFilter(\n",
    "            embeddings=self.embeddings)\n",
    "        reranker = FlashrankRerank()\n",
    "        pipeline_compressor = DocumentCompressorPipeline(\n",
    "            transformers=[redundant_filter, reranker])\n",
    "        compression_pipeline = ContextualCompressionRetriever(\n",
    "            base_compressor=pipeline_compressor,\n",
    "            base_retriever=ensemble_retriever\n",
    "        )\n",
    "\n",
    "        return compression_pipeline\n",
    "\n",
    "    def invoke(self, query, compression_pipeline):\n",
    "        \"\"\"\n",
    "        Invokes the ensemble retriever with reranker and preprocesses the results.\n",
    "\n",
    "        Args:\n",
    "            query: The query string.\n",
    "            compression_pipeline: The ensemble retriever with reranker.\n",
    "\n",
    "        Returns:\n",
    "            list[Document]: List of Document objects with all numpy.float32 values converted to float.\n",
    "        \"\"\"\n",
    "        # Use the `invoke` method to retrieve documents\n",
    "        docs = compression_pipeline.invoke(query)\n",
    "\n",
    "        # Process each document to ensure all numpy.float32 values are converted to float\n",
    "        processed_docs = [self._process_document(doc) for doc in docs]\n",
    "\n",
    "        return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "instruction = \"\"\"The provided document is a PDF file containing structured and unstructured content.\n",
    "It may include financial information, tables, management discussions, and analyses.\n",
    "Try to capture the essence of the document, including text, tables, and key highlights.\n",
    "Be precise and ensure data integrity while processing.\"\"\"\n",
    "\n",
    "\n",
    "async def parse_pdf(file_path: str):\n",
    "  parser = LlamaParse(\n",
    "      result_type=\"markdown\",\n",
    "      parsing_instruction=instruction,\n",
    "      max_timeout=5000,\n",
    "  )\n",
    "  return await parser.aload_data(file_path)\n",
    "\n",
    "\n",
    "async def load_and_combine_documents(folder_path: str, output_folder: str):\n",
    "  for filename in os.listdir(folder_path):\n",
    "    combined_content = \"\"\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if filename.endswith('.pdf'):\n",
    "        print(f\"Parsing {filename}...\")\n",
    "        parsed_data = await parse_pdf(file_path)\n",
    "        combined_content += f\"# Document: {filename}\\n\\n{parsed_data}\\n\\n\"\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {filename}\")\n",
    "    output_file = output_folder + \"/\" + os.path.splitext(filename)[0] + \".md\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as md_file:\n",
    "        md_file.write(combined_content)\n",
    "    print(f\"All documents combined into {output_file}\")\n",
    "\n",
    "\n",
    "def read_markdown_with_loader(folder_path: str):\n",
    "  documents = []\n",
    "  for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if filename.endswith('.md'):\n",
    "      loader = UnstructuredMarkdownLoader(file_path)\n",
    "      documents.append(loader.load())\n",
    "  return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Insatiate RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported file type: .DS_Store\n",
      "All documents combined into ./dataset/converted_md/.DS_Store.md\n",
      "Parsing Diabetes-Whitepaper.pdf...\n",
      "Started parsing the file under job_id ae7bb38a-5c41-4700-9d7a-9b20d278e080\n",
      "All documents combined into ./dataset/converted_md/Diabetes-Whitepaper.md\n",
      "Parsing Erratum_jand.pdf...\n",
      "Started parsing the file under job_id 7fdf4a08-cf47-4e37-9512-cfaae0f03fea\n",
      "All documents combined into ./dataset/converted_md/Erratum_jand.md\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./dataset/source\"\n",
    "output_folder = \"./dataset/converted_md\"\n",
    "await load_and_combine_documents(folder_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = read_markdown_with_loader(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_contextual_rag = False\n",
    "my_rag = MyRAGPipeline(create_contextual_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-3B-Instruct\")\n",
    "tpm_limit = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_chunks(document, max_retries=1, delay=60):\n",
    "    details = {}\n",
    "    source_document = document[0].metadata[\"source\"]\n",
    "    details[\"source\"] = source_document\n",
    "    details[\"original_chunks\"], details[\"contextualized_chunks\"] = my_rag.split_document(\n",
    "        document, max_retries, delay)\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: ./storage/bm25_index\n"
     ]
    }
   ],
   "source": [
    "index_manager = WhooshIndexManager(index_dir=\"./storage/bm25_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_token_limit(document, current_tokens, tpm_limit):\n",
    "    tokens = len(tokenizer.encode(str(document)))\n",
    "    current_tokens += tokens\n",
    "\n",
    "    if current_tokens >= tpm_limit:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document, retry_document_processing, current_tokens, delay=60):\n",
    "  try:\n",
    "    source_document = document[0].metadata[\"source\"]\n",
    "    print(f\"Processing document: {source_document} with currrent tokens: {current_tokens}\")\n",
    "    details = get_document_chunks(document, 1, delay)\n",
    "\n",
    "    if bool(details):\n",
    "        my_rag.create_vectorstores(details[\"original_chunks\"])\n",
    "        index_manager.add_documents(details[\"original_chunks\"])\n",
    "        if create_contextual_rag:\n",
    "            contextualized_vectorstore = my_rag.create_vectorstores(details[\"contextualized_chunks\"])\n",
    "            index_manager.add_documents(details[\"contextualized_chunks\"])\n",
    "\n",
    "        # bm25_retriever_original = my_rag.create_bm25_retriever(details[\"original_chunks\"])\n",
    "        # bm25_retriever_contextualized = my_rag.create_bm25_retriever(details[\"contextualized_chunks\"])\n",
    "        # original_ensemble_retriever_reranker = my_rag.create_ensemble_retriever_reranker(original_vectorstore, bm25_retriever_original)\n",
    "        # contextualized_ensemble_retriever_reranker = my_rag.create_ensemble_retriever_reranker(contextualized_vectorstore, bm25_retriever_contextualized)\n",
    "\n",
    "    if create_contextual_rag and not is_within_token_limit(details, current_tokens, tpm_limit):\n",
    "        print(f\"TPM limit reached, current tokens are: {current_tokens}. Waiting for {delay} seconds...\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return current_tokens\n",
    "\n",
    "  except Exception as e:\n",
    "      print(f\"Failed to process document: {source_document}. Error: {e}\")\n",
    "      if delay > 60: # Hack: it's retrying, don't append again\n",
    "        retry_document_processing.append(document)\n",
    "\n",
    "      return current_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document: ./dataset/converted_md/Erratum_jand.md with currrent tokens: 0\n",
      "Total number of chunks in document: 10\n",
      "Creating new FAISS index...\n",
      "Saving FAISS index...\n",
      "Adding 10 documents to the FAISS index...\n",
      "Added 10 documents to the index.\n",
      "Processing document: ./dataset/converted_md/Table-of-Contents_jand.md with currrent tokens: 2168\n",
      "Total number of chunks in document: 12\n",
      "Adding 12 documents to the FAISS index...\n",
      "Added 12 documents to the index.\n",
      "Processing document: ./dataset/converted_md/Diabetes-Whitepaper.md with currrent tokens: 4970\n",
      "Total number of chunks in document: 5\n",
      "Adding 5 documents to the FAISS index...\n",
      "Added 5 documents to the index.\n",
      "Processing document: ./dataset/converted_md/July-2018-People-&amp;-Events_jand.md with currrent tokens: 5863\n",
      "Total number of chunks in document: 5\n",
      "Adding 5 documents to the FAISS index...\n",
      "Added 5 documents to the index.\n",
      "Processing document: ./dataset/converted_md/Inuit-Country-Food-Diet-Pattern-Is-Associated-with.md with currrent tokens: 6936\n",
      "Total number of chunks in document: 54\n",
      "Adding 54 documents to the FAISS index...\n",
      "Added 54 documents to the index.\n",
      "TPM limit reached, current tokens are: 19699. Waiting for 22 seconds...\n",
      "Processing document: ./dataset/converted_md/July-2018-Classified-Advertisements_jand.md with currrent tokens: 0\n",
      "Total number of chunks in document: 5\n",
      "Adding 5 documents to the FAISS index...\n",
      "Added 5 documents to the index.\n",
      "Processing document: ./dataset/converted_md/What-Is-a-National-Provider-Identifier-and-Why-Doe.md with currrent tokens: 858\n",
      "Total number of chunks in document: 6\n",
      "Adding 6 documents to the FAISS index...\n",
      "Added 6 documents to the index.\n",
      "Processing document: ./dataset/converted_md/The-Value-Proposition-of-Academy-Membership_2018_j.md with currrent tokens: 1902\n",
      "Total number of chunks in document: 5\n",
      "Adding 5 documents to the FAISS index...\n",
      "Added 5 documents to the index.\n",
      "Processing document: ./dataset/converted_md/AMA Perspective Paper 1 Prevention in VBC 102716.md with currrent tokens: 2730\n",
      "Total number of chunks in document: 19\n",
      "Adding 19 documents to the FAISS index...\n",
      "Added 19 documents to the index.\n",
      "Processing document: ./dataset/converted_md/What's-New-Online_jand.md with currrent tokens: 6227\n",
      "Total number of chunks in document: 5\n",
      "Adding 5 documents to the FAISS index...\n",
      "Added 5 documents to the index.\n",
      "TPM limit reached, current tokens are: 7041. Waiting for 22 seconds...\n",
      "Processing document: ./dataset/converted_md/ColPali_2407.01449v3.md with currrent tokens: 0\n",
      "Total number of chunks in document: 81\n",
      "Adding 81 documents to the FAISS index...\n",
      "Added 81 documents to the index.\n",
      "TPM limit reached, current tokens are: 18369. Waiting for 22 seconds...\n"
     ]
    }
   ],
   "source": [
    "retry_document_processing = []\n",
    "current_tokens = 0\n",
    "\n",
    "for document in documents:\n",
    "  current_tokens = process_document(document, retry_document_processing, current_tokens, 22)\n",
    "\n",
    "# Retry processing failed documents\n",
    "if retry_document_processing:\n",
    "    print(\"Retrying failed documents...\")\n",
    "    for document in retry_document_processing:\n",
    "        current_tokens = process_document(document, retry_document_processing, current_tokens, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving FAISS index...\n"
     ]
    }
   ],
   "source": [
    "my_rag.save_vectorstores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_reranker = cr.create_flashrank_index(original_vectorstore)\n",
    "# contextualized_reranker = cr.create_flashrank_index(contextualized_vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create retriver system with hybrid search with Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing index from: ./storage/bm25_index\n",
      "Loading existing FAISS index...\n"
     ]
    }
   ],
   "source": [
    "index_manager = WhooshIndexManager(index_dir=\"./storage/bm25_index\")\n",
    "\n",
    "bm25_retriever = WhooshRetriever(index_manager=index_manager, k=10)\n",
    "vectorstore = my_rag.get_vectorstores()\n",
    "ensemble_retriever_reranker = my_rag.create_ensemble_retriever_reranker(vectorstore, bm25_retriever)\n",
    "\n",
    "# # Retrieve documents based on a query\n",
    "# query = \"second document\"\n",
    "# results = bm25_retriever._get_relevant_documents(query)\n",
    "# for result in results:\n",
    "#     print(result.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
      "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
      "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 5, 'relevance_score': 0.9874792, 'source': './dataset/converted_md/ColPali_2407.01449v3.md'}, page_content='and textual representations using contrastive losses, though OCR capabilities may be limited.\\\\n- Visually Rich Document Understanding: Models that encode text alongside visual features to enhance understanding.\\\\n- PaliGemma Model: A model that combines text and image embeddings for improved performance in tasks like Visual Question Answering and document understanding.\\\\n\\\\n5. ViDoRe Benchmark:\\\\n- A new benchmark designed to evaluate document retrieval systems at the page level, focusing on various modalities (text, figures, tables) and thematic domains (e.g., medical).\\\\n\\\\n### Structured Content:\\\\n\\\\n#### 1. Neural Retrievers\\\\n- Bi-encoder Models:\\\\n- Independent document mapping to dense vector space.\\\\n- Fast cosine distance computation for query matching.\\\\n\\\\n- Cross-encoder Systems:\\\\n-'),\n",
       " Document(metadata={'id': 0, 'relevance_score': 0.98633367, 'source': './dataset/converted_md/ColPali_2407.01449v3.md'}, page_content='late interaction engines, many popular frameworks lack native multi-vector support, necessitating engineering efforts to adapt them for use with ColPali or ColBERT models.\\\\n\\\\n3. Data: The creation of the ViDoRe benchmark partially relies on synthetic query generation from a commercial large language model, which may introduce bias. To mitigate this, the prompting strategy has been refined, and real query examples were provided to ground the generation in realistic contexts. All synthetic queries underwent manual verification to ensure relevance and quality. The benchmark also includes numerous tasks without synthetic data, with observed result trends across tasks confirming the coherence of the benchmark design.\\\\n\\\\n### Ethical Considerations\\\\n\\\\n- Carbon Footprint: The work leverages'),\n",
       " Document(metadata={'id': 4, 'relevance_score': 0.9786109, 'source': './dataset/converted_md/ColPali_2407.01449v3.md'}, page_content='the challenges faced by modern document retrieval systems, particularly their inefficiency in utilizing visual cues from documents. To address this, the authors introduce the Visual Document Retrieval Benchmark (ViDoRe), which includes various tasks across different domains and languages. They propose a new retrieval model architecture, ColPali, which utilizes Vision Language Models to create high-quality contextual embeddings from document images. ColPali, combined with a late interaction matching mechanism, significantly outperforms existing document retrieval systems while being faster and end-to-end trainable. All project artifacts are available at Hugging Face.\\\\n\\\\nIntroduction:\\\\nDocument retrieval involves matching user queries to relevant documents, crucial for applications like')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retriever_reranker.invoke(\"What is ViDoRe?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "ensemble_reranker = EnsembleRetrieverReranker(embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_pipeline = ensemble_reranker.create_ensemble_retriever_reranker(vectorstore, bm25_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
      "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n",
      "ERROR:langsmith._internal._serde:Failed to use model_dump to serialize <class 'langchain_core.documents.base.Document'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'numpy.float32'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and textual representations using contrastive losses, though OCR capabilities may be limited.\\n- Visually Rich Document Understanding: Models that encode text alongside visual features to enhance understanding.\\n- PaliGemma Model: A model that combines text and image embeddings for improved performance in tasks like Visual Question Answering and document understanding.\\n\\n5. ViDoRe Benchmark:\\n- A new benchmark designed to evaluate document retrieval systems at the page level, focusing on various modalities (text, figures, tables) and thematic domains (e.g., medical).\\n\\n### Structured Content:\\n\\n#### 1. Neural Retrievers\\n- Bi-encoder Models:\\n- Independent document mapping to dense vector space.\\n- Fast cosine distance computation for query matching.\\n\\n- Cross-encoder Systems:\\n-\n",
      "{'id': 5, 'relevance_score': 0.9874792098999023, 'source': './dataset/converted_md/ColPali_2407.01449v3.md'}\n",
      "late interaction engines, many popular frameworks lack native multi-vector support, necessitating engineering efforts to adapt them for use with ColPali or ColBERT models.\\n\\n3. Data: The creation of the ViDoRe benchmark partially relies on synthetic query generation from a commercial large language model, which may introduce bias. To mitigate this, the prompting strategy has been refined, and real query examples were provided to ground the generation in realistic contexts. All synthetic queries underwent manual verification to ensure relevance and quality. The benchmark also includes numerous tasks without synthetic data, with observed result trends across tasks confirming the coherence of the benchmark design.\\n\\n### Ethical Considerations\\n\\n- Carbon Footprint: The work leverages\n",
      "{'id': 0, 'relevance_score': 0.9863336682319641, 'source': './dataset/converted_md/ColPali_2407.01449v3.md'}\n",
      "the challenges faced by modern document retrieval systems, particularly their inefficiency in utilizing visual cues from documents. To address this, the authors introduce the Visual Document Retrieval Benchmark (ViDoRe), which includes various tasks across different domains and languages. They propose a new retrieval model architecture, ColPali, which utilizes Vision Language Models to create high-quality contextual embeddings from document images. ColPali, combined with a late interaction matching mechanism, significantly outperforms existing document retrieval systems while being faster and end-to-end trainable. All project artifacts are available at Hugging Face.\\n\\nIntroduction:\\nDocument retrieval involves matching user queries to relevant documents, crucial for applications like\n",
      "{'id': 4, 'relevance_score': 0.9786108732223511, 'source': './dataset/converted_md/ColPali_2407.01449v3.md'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is ViDoRe?\"\n",
    "results = ensemble_reranker.invoke(query, compression_pipeline)\n",
    "\n",
    "for doc in results:\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
